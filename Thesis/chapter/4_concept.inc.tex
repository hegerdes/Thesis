% !TeX root = ../thesis_main.tex

\section{The Concept of DevContainers}\label{sec::solution_concept}
This section proposes the usage of Development Containers (DevContainers) as a solution to the problems of local development environments as described in section \ref{sec::problem}. The concept was developed oriented towards the \ac{PaaS} principles predominant in the cloud. Developers should only have to deal with the application to be developed. The runtime environment and all configuration related tasks are managed by the DevContainer. They combine the application runtime and its configuration into an isolated environment by using the lightweight virtualization approach of containers. One difference between DevContainers and the \ac{PaaS} principle is that DevContainers provide neither own hardware, nor is it management by and third party entity.\newline
The details of such a DevContainer concept are described below. Followed by a delimitation of the cases, in which its application scenarios are sense and when they are not. Finally, the advantages and limitations of this solution are outlined.

    \subsection{Description of a Conceptual Environment}
    The idea of DevContainers is to bundle the application code, its runtime and configuration into an isolated system. Only the minimum necessary scope for accessing the applications functionality is made available on the host system, all other resources remain isolated. According to the design of \ac{VM}s and containers, they can be started and stopped at will without encountering the risk of permanent changes on the host system by the containerized applications. When the container is started, all port, path and secret configurations are already set. Developers are not required to do any further manual configuration. Different versions and branches of an application have their own container, which are completely independent of each other and the host. Auxiliary applications and interdependent services can all be started simultaneously and in the correct order. The DevContainer environment builds upon the container implementation Docker and thus, is completely independent of the host \ac{OS}. Since Docker is Linux based, the development environment is thereby much closer to the production environment, potentially decreasing system-specific errors. Even in case of an error and the environment ends up in an undefined state, the principles of containers can be applied. The DevContainer can simply be discarded and recreated from a well-defined template within seconds. This design enables environments that are uniform and 100\% reproducible.\newline
    Such a setup allows developers to choose any host \acl{OS} because the entire application code is executed in a virtualized environment. It increases the initial setup time and prevents configuration drift since all configuration settings follow the \ac{IaC} principle and are stored as code. This way, new runtimes or dependencies can be tested without the risk to corrupt the local development environment. Through automatic orchestration, integration tests can be performed easier and earlier, decreasing the time until an error is detected. Dependencies and common software like debugger are already present in the container, so they do not need to be installed separately. This newly designed concept of DevContainers for local development environments promise to solve the problems described in section \ref{sec::problem} and allow developers to focus on programming rather than configuring and maintaining their working environment.

    \subsection{Pre-requirements for DevContainers}
    Before using DevContainers, one has to verify that this approach is the appropriate strategy for solving the problems encountered. DevContainers are based on virtualization technology, and one of their goals is to achieve the greatest possible similarity between the development and the production environment. In order to take full advantage of this possibility, the production environment must already be designed for the use of virtualization with containers. As can be seen in section \ref{ssec::toolsused}, the majority of virtualization solutions are based on Linux, which is also the most widely used system in the server domain. Applications that require Windows can also use Windows-based containers, but these may require additional licenses and configuration, accordingly they will not be discussed any further in this paper. The application to be developed must be suitable for the use in a container. Containers do not offer direct support of graphical output by default. Functions are exposed to the outside world via TCP/UDP sockets or mounted devices. Building the base images for DevContainer takes time, so this should not be performed by every developer, a functioning \ac{CI}/\ac{CD} pipeline for the rapid delivery of new images is therefore also recommended.\newline
    Although, DevContainers reduce the configuration effort for each developer, the architecture and settings files for the use of DevContainers must be created and then be maintained. As section \ref{sssec::virtual} has already shown, any kind of virtualization, regardless of whether it is VM-based or container-based, there is a performance overhead. If, as in a typical microservice architecture, several applications are run at the same time, this overhead adds up and places an additional load on the developer's system. Modern hardware with sufficient memory and computing capacity is therefore necessary for the use of DevContainers. The amount of additional load depends on the technology stack used. The exact effects of this overhead are described in more detail in section \ref{sss::eval_proto}.

    \subsection{Creating a DevContainer setup}\label{ssec::toolsused}
    Section \ref{ssec::getting_devops} has already presented concrete programs for providing the virtualization concept and automating certain processes with a \ac{CI} system. The usage of these existing tools built the foundation for provisioning DevContainers. Developers need to be able to initialize a DevContainer based environment quickly and must be offered a convenient way to interact with the DevContainer, and especially the application within, without an impacting on their workflow. How the tools described above are used beyond their intended purpose and what tactics are employed to solve the problems described in section \ref{sec::problem} is described in the following section.

        \subsubsection{Defining the Application Runtime}
        The entire application runtime environment is virtualized by using Docker. However, the basics of this runtime environment must be defined beforehand. In the case of Docker, this is done with Dockerfiles. These provide the instructions for building Docker-images, which are needed for a container in order to start. \ac{CI} platforms like GitLab typically execute the building process for images automatically on every new application version \cite{docker2020}.\newline
        \input{inc/code/code_docker_dev_example.inc}
        Listing \ref{code::docker_dev_example} shows such a Dockerfile for a Python application. The Linux distribution Debian Buster is chosen as the starting point for the image, followed by instructions for installing all required dependencies and programs, including the linter \code{pep8}. When it comes to the source code, there are two possibilities for implementing the DevContainer concept. Optionally, the program code can be copied into the image so that the image already contains all the program components. The resulting fully self-contained image is the typical method in production operation. The disadvantage of this approach is that the program code can only be accessed within the container and is not directly present on the host system. While this is desirable in production, due to security and strict isolation, it may not be ideal for development purposes. Developers are limited by only using applications installed within the container to edit the code, and the \ac{CI} system must create new images for every commit. Prematurely discarded containers can lead to the loss of changes that have not yet been published to the remote source code repository \cite{dockerdocs}. This would definitely impose restrictions on the developers' workflow.\newline
        Alternatively, copying the source code can be omitted and only the well-defined runtime environment is provided by the image. At runtime, the source code stored on the host is then mapped into the container via a bind-mount when the container is started. This way, the source code can be edited and used by any editor or program on the host. In the case of a discarded container, the changes made are still available on the host. Furthermore, a new image only needs to be created when a change in the Dockerfile or dependencies occurs, which saves significant computational effort in the \ac{CI} system. This developed approach allows simultaneous use of local tools and the programs inside the container. The workflows of the developers are not restricted nor even significantly changed.\newline
        After the build process of an image is completed it is made available on a private or public image registry in order to be accessible. In the working directory of a project, the following commands are executed by the \ac{CI} system in order to create, name and upload the image to a registry:

\begin{lstlisting}[language=bash, frame=none, numbers=none, backgroundcolor=\color{codebg}]
docker build -t my-python-app .
docker push my-python-app
\end{lstlisting}
\vspace{-1cm}

        \subsubsection{Orchestrating the Application Containers}
        In order for the Python application above to work, it needs a database. Instead of each developer having to install and set up a database on their own, which may then shared between different projects, this can be done in isolation and automatically for each project by using virtualization and composition. Applications in a \ac{MSA} have multiple services they depend on. Arranging all these programs in order to create one greater service is called orchestration. Well-known applications for container-based orchestration are \wordhighlight{Docker-Compose} and \wordhighlight{Kubernetes}. Since Kubernetes does not ship with Docker by default, is quite complex and production oriented, a performed comparison showed that Docker-Compose is the recommended choice for local orchestration.\newline
        Docker-Compose requires a configuration file, declared in the \acs{YAML} format, which contains all the necessary information for orchestrating multiple applications. Each application is defined as a service with a unique name when using Docker-Compose. The \code{docker-compose.yml} file in Listing \ref{code::compose_example} defines the services \code{app} and \code{db}. Each service has an image, providing the initial state for each container. With Docker containers, it is common to use environment variables to influence and configure the application within the container. The credentials for the database and the Python app are set via environment variables. It should be noted that the BD host can just be the service name of the database server instead of an IP address. Within the Docker managed network, the integrated \ac{DNS} server automatically resolves all service names to the appropriate IP addresses of each container. This virtual network allows containers to communicate with each other. In order access the containers functionality on the host, the network ports used must be explicitly exposed to the host. The Python application uses the alternative \acs{HTTP} port \code{8080} and the database server uses the standard MySQL port. When starting the containers, the corresponding ports are allocated on the host system and local programs can access the defined services at \code{localhost} and the corresponding network port \cite{dockerdocs}.\newline
        If the source code is not copied into the image, it must be mapped into the container using bind-mounts. Line 12 in Listing \ref{code::compose_example} binds the local \code{app-src} directory of the host to the container at the location \code{/workspace}. The database service, on the other hand, uses a volume mount, in which docker manages the allocated storage persistently itself. The configuration is completed with an entrypoint for the Python app that is executed when the container starts.\newline
        Docker-Compose ensures that all interdependent services are started in the correct order, that all persistent volumes are created, and that all containers can communicate within their network \cite{docker2020}, \cite{dockerdocs}. Since all the necessary configurations are stored in an ordinary text file, the \ac{IaC} principle applies here. The state of the \code{docker-compose.yml} file and all changes are tracked in \ac{VCS}.\newline
        \input{inc/code/code_compose_example.inc}
        The command \code{docker-compose up} is used in order to start all services specified in the \code{docker-compose.yml} file. In case the docker images are yet available on the host, they are automatically downloaded from image-registries. When all services are started, Docker-Compose attaches itself to all running containers and writes the color-coded output of the every program to the console.\newline
        Through this concept, developers have isolated program environments that are created quickly, easily and are producible. Multiple separated projects, no longer have to share a database and different language frameworks, interpreters and compilers can be tested independently. In case an additional service, like a fast cache-server is needed, another service can be created quickly in order to provide a \wordhighlight{Redis} or \wordhighlight{Memcached} server without the need of manually installing it to the host. Individual, manual and diverging configurations for each developer are eliminated. New environments that are similar to the production environment can be created quickly, independent of the hosts operating system. In a microservice architecture, developers have the opportunity to test the interaction of multiple applications before committing changes to the \ac{VCS}. This way, potential errors are already detected before \ac{CI} integration tests, which means that they can be corrected more quickly. These characteristics are promising in terms of solving the problems described in section \ref{sec::problem} regarding heterogeneity, lack of testing facilities, and the tedious configuration of the setup. In order for this concept to be adopted by developers, there must be an equally effective way for developers to interact with their applications within the DevContainer.

        \subsubsection{Interacting with DevContainers}
        The primary way developers interact with their code and the application is through their editor. Their variability and the number of different editors is great. \wordhighlight{Visual Studio, XCode, Atom, Sublime, Eclipse, Emacs} and \wordhighlight{VIM} are well-known general purpose editors. There is usually a distinction between simple text editors and \acl{IDE}. While text editors are quite simple and only provide basic functionalities like syntax highlighting, \ac{IDE}s are much more comprehensive with powerful IntelliSense suggestions, built-in project management, a \ac{VCS}, debugger, graphical visualization and build tools. The choice of the editor is a personal decision for most developers, and they are customized according to their preferences. For this reason, the proposed solution developed in this thesis does not require a specific editor for DevContainers, but gives a recommendation which will be used as a reference for the rest of the work.
        \begin{figure}
            \centering
            \captionsetup{justification=centering}
            \includegraphics[width=.95\textwidth]{architecture-containers.png}
            \caption{Architecture of \ac{VSCode} Development Container Setup, \\\textit{Source:~\cite{vscodedevcontainer}}}\label{fig::vscodecontainer}
        \end{figure}
        \ac{VSCode} is a free and platform independent editor with extensive extendibility, which is used by over 70\% of all developers accordingly to StackOverflow~\cite{stackoverflow2021}. One of its beneficial features for virtualized workloads is its remote development functionally. This officially provided, extension allows developers to use the intuitive comfort of a graphical \ac{UI} while running the code, the application and auxiliary processes like a debugger on another, remote machine. Figure \ref{fig::vscodecontainer} shows how the \ac{VSCode} frontend connects to a remote machine or container and installs a server instance of the editor. The server side instance manages the access to the remote file system and the execution of processes while communicating with the local \ac{VSCode} frontend instance for comfortable access to these functions. This type of remote development works for \ac{SSH} connections, the \ac{WSL}, and on Docker containers. \ac{WSL} is a built-in Windows feature to provide a Linux environment on Windows hosts without the need of a separated \ac{VM}. The implementation of Docker for Windows is build upon this feature \cite{vscodedevcontainer}. In the further course, only the remote functions for containers will be considered here.\newline
        In order t access a containerized application, the appropriate ports need to be exposed via Docker-Compose. Even though \ac{VSCode} provides a similar feature through the remote extension, it was concluded that this function is unstable. It allows making remote processes available locally through port forwarding. \ac{VSCode}s implementation can not expose ports for multiple containers simultaneously, making the feature only useful for a service that are currently worked on, or for services with dynamically changing network ports. However, this feature can be used to make a short-lived process available on the host quickly without having to modify the configuration in the \code{docker-compose.yml} file. However, Dockers port mapping functionality is still the primarily used way to interact with the containerized application.\newline
        In order for \ac{VSCode} to connect to or start a DevContainer, a configuration file is required. This is provided by a \code{devcontainer.json} file. For each service developed, the path to the \code{docker-compose.yml} file is specified, as well as the service name \ac{VSCode} should connect to. Listing \ref{code::devcontainer_json} shows such a configuration file. It also specifies, which directory the \ac{VSCode} server should open, which extensions will be installed and what happens when the container starts or stops. If this file exists, \ac{VSCode} automatically offers to reload the local project using the DevContainer.
        \vspace{.2cm}
        \input{inc/code/code_devcontainer_json.inc}
        \noindent All auxiliary services are started automatically, directories are mounted, and the network ports are allocated accordingly. Thus, the development work is nearly identical to a local setup \cite{vscodedevcontainer}.\newline
        All these functions can be archived without \ac{VSCode} by using, (\ac{SSH}) port-forwarding or terminal-based editors. Even without VSCode, any file change made by any editor on the host will take effect, since the source code directories are bind-mounted into the container.

        \subsubsection{Variations and Additional Supporting Tools}
        In addition to the tools mentioned above and in section \ref{ssec::getting_devops}, other auxiliary programs can be used. To simplify certain workflows and automate recurring tasks, scripts will be used. The scripting language Bash can be used natively on Linux and macOS, the installation of Git for Windows also brings Bash support to Windows. Accordingly, one uniform scripting language can be used to perform platform-independent operations and to simplify complex instructions. In order for developers not having to wait for the creation of the DevContainer images, these should be built automatically by a \ac{CI}-tool and made available in a private container registry. Accordingly, uniform and up-to-date images are quickly available to all developers. \newline
        It is also possible to use other management tools via Docker. Database management tools, such as \wordhighlight{phpMyAdmin}, can simplify administration by adding further services. Docker's capabilities allow multiple instances of these tools to run in isolation from each other in different versions without creating conflicts. Although Docker-Compose offers a color-coded log output, even this can become overwhelming if there are too many logs from multiple applications. Since the Docker stack is used anyway, enterprise log aggregators and analysis tools like \wordhighlight{Grafana} or \wordhighlight{Elastic-Search} can be used to get a persistent and searchable log dashboard. No program outputs will ever be lost and are easy to filter and search through. Developer teams can have a central log server that everyone can access. In case of a bug, team members can directly see the error messages from others and offer appropriate help. This developed concept even allows to mirror the entire production service structure in a local environment. The configuration of a Grafana dashboard or others will not be discussed any further in this paper, as this is beyond the scope.

    \subsection{Security Aspects of this Concept}\label{ssec::sec}
    The presented solution is oriented towards the development process and not for productive operation. For this reason, common security practices for container operation are not implemented. The images contain extra programs that are not mandatory, but convenient. The processes in the container run with super-user privileges to any avoid additional configuration effort and interruptions due to permission errors. In the \code{docker-compose.yml} file, passwords are defined in plain text in order to allow a consistent and easy setup process across all systems. Functions and data within the containers are made available through exposed ports on the host. Accordingly, it must be ensured that the host is not accessible from the Internet. If certain services, such as databases or log dashboards, are shared between developers via a central server, it must be ensured that this server is only accessible within the company network in order to avoid the unintentional publication of confidential information. The same security measures must be applied for a non-containerized local development environments.

    \subsection{Strengths, Weaknesses and Limits}\label{ssec::limits}
    The software described in section \ref{ssec::toolsused} are already used in DevOps enabled teams. Accordingly, the entry barrier is small compared to new and unknown tools. DevContainer allow for a homogenization of development and production environments. The application runtime is identical, accordingly \ac{OS} or runtime specific errors are prevented. Developers can use a ready-to-go development project setups without having to install and configure the application runtime themselves. Dependencies, keys, supporting tools like debuggers, and configurations can already be shipped within the container to enable a quick initial setup. Instead of examining the environment for a long time in the event of an error, it can quickly be torn down and recreated to a known good state. The possibility to start a large service, consisting of many applications, with one command improves the workflow and allows for much more extensive testing possibilities. These are the key features that DevContainer promise to provide, in addition to solutions to the problems described in section \ref{sec::problem}. They extend the existing toolset of agile and DevOps teams by another tool that allows developers to be more flexible and to focus more on coding.\newline
    It should be noted that DevContainers are not a perfect solution to all problems in the software development stack. Like any other tool, they come with their own set of quirks. Expertise for the use of Docker containers must be available, and the production architecture must be transformable to a local DevContainer-based configuration. This configuration must be kept up to date and maintained. Developers may need to adapt to minor adjustments in their workflow. The \ac{CI}/\ac{CD} solution used must always be available and provide up-to-date container images. Every virtualization approach creates an additional overhead that cannot be ignored, especially when using multiple containers on Windows based systems. Details on the exact effects are given in section \ref{sec::eval}.\newline
    As mentioned before, there are also projects, for which the use of DevContainers is not suitable. Heavy monolithic applications are not in the sense of containers and therefore not ideal for DevContainers. Graphical applications can run in containers, but require a graphical X-Server on the host, so that the resulting experience is not comparable to a native \ac{UX}. Similar limitations apply to applications which require a Windows stack. Containers based on Windows do exist, but they require a Windows host, accordingly they are no longer platform-independent, require additional licenses and further adaptations. For embedded projects that require specific unconventional hardware, a virtualization approach may not feasible, since virtualization abstracts the hardware layer.\newline
    In order to demonstrate an appropriate use of DevContainers, the next bigger section describes how to migrate from a traditional development environment to a DevContainer-based solution based on a real project. Even if this concept has not yet found much consideration in the scientific literature, there are efforts from the commercial sector, which have recognized similar problems as described in section \ref{sec::problem} and try to offer solutions with their products. The growing number of these providers shows that there is economic interest in this area and that these firemen expect a growing market here. A selection of these commercial solutions is presented in the following.

    \subsection{Alternative Solutions}\label{ssec::alternatives}
    Apart from the concept presented here, there are already alternative solutions on the market which promise to solve problems similar to the ones described in section \ref{sec::problem}. A selection of these solutions is presented in the following section. These alternative development environments can roughly be classified into two categories: browser-based and container-based solutions. A comparison in terms of functionality of presented concepts will be in section \ref{sses::eval_compare}.
    \myparagraph{Browser-Based Development Environments}
    Browser-based development solutions provide an editor that is entirely built upon web technologies like HTML, CSS and \ac{JS}. Thereby, they can run on every device with a modern web browser. Their goal is to provide a quick functioning setup without any configuration.\newline
    The products CodeSandbox.io and Stackblitz implement this approach. Both solutions offer a \ac{VSCode}-like editor in the browser and allows the development of NodeJS-based JavaScript projects. The functionality of codesandbox.io is provided by a client-server architecture where the browser-editor is the client and the server is managed by CodeSandbox itself. Therefore, Developers have little control over the actual runtime and always need an active internet connection. Stackblitz, on the other hand, can also be used without an Internet connection. Stackblitz is a \ac{PWA}, which brings the NodeJS runtime into the browser by using Webassembly. Accordingly, Stackblitz does not need a backend-server, because it is already running in the browser. While codesandbox.io does not provide any console at all and is completely managed by the service provider, Stackblitz provides a minimal shell for installing, copying and launching files. However, since both solutions run within the browser, regular TCP/UDP connections are restricted, due to browser security regulations. Network connections are only made available via HTTP or the HTTP-based WebSockets protocol \cite{codesandbox}, \cite{stackblitz}.

    \myparagraph{Container-Based Development Environments}
    Similar to the solution presented in this paper, this approach uses containers in, in which everything can be installed theoretically. Compared to browser-based solutions, container-based development environments offer broader functionality and are not restricted to one specific programming language. The source code of an application and all the dependencies it takes to run it are bundled within one container. The best-known products that implement this are Gitpod and GitHub's Codespaces, which was not released until August 2021. The infrastructure for the containers is offered by the provider, but Gitpod also offers a self-hosting option. There are no restrictions for users in the containers, additional software can be added as desired, and regular network ports can also be opened to the outside. Both solutions can be used in the browser as well as in a local \ac{VSCode} installation via the remote development extension. An active internet connection is required in order to use these services. Both services are billed either on a monthly basis or on an hourly basis \cite{githubcodespace}, \cite{gitpod}.
