% !TeX root = ../thesis_main.tex


\section{Analysis of the Current State of Development Environments}\label{sec::problem}
The following section will display the current status of a typical development environment, point out recurrent problems and provide approaches for possible solutions.

    \subsection{Current State of Development Environments}
    Software development involves a broad variety of tasks. Depending on the project, tasks can range from web development, embedded system development, desktop or mobile application development, data analysis or the pure maintenance of one of these areas. Each of these software development fields has its own workflows and requirements for the actual development setup. Even within these specialized categories, there are different requirements, depending on the scope, size, and general demand for capabilities and power of a project. Accordingly, the setups of development environments can differ greatly from one another. Due to the large scope of the subject and in order to set useful limitations, this work will only refer to web services built on a microservice architecture. Nevertheless, parts of this work can also be adapted to other types of development.\newline
    Despite the increasing use of web-based \ac{IDE}s, such as \wordhighlight{CodeSandbox}, \wordhighlight{StackBlitz}, \wordhighlight{Codespaces} and \wordhighlight{Gitpod}, native development environments are still used primarily. The StackOverflow Survey 2021 supports this thesis and clarifies further that Windows is the primarily used operating system, among professional developers, followed by macOS and Linux~\cite{stackoverflow2021}. Accordingly, developers need modern hardware, on which all the required applications must be installed and set up locally. Depending on the application area, this may include some applications for which license costs may have to be paid and which must be kept up to date after the initial installation. The setup and maintenance of these tasks are necessary supporting processes in software development, but they can cost a lot of time and effort, and therefore money without producing any actual progress and value. This is especially true if there are many developers in a large company with frequently changing developers. Once the environment is running, developers can start to code and contribute to the project. Programming is about changing things and changes can break things, as can be seen in the case of GitHub.com. Here, developers created a series of scripts just to get developers to a working environment in less than a day. Because of frequent error these scripts included a global clean option (called \code{--nuke-from-orbit}) to reset the environment the initial, known good, state~\cite{githubblogcodespace}.\newline
    Such a local working environment leaves developers in a position, in which they have more control over their development environment than in the case of the pre-defined and functionally restricted web-based environments.
    However, local setups require much effort and are result in an elaborately error-proneness's environment as the next section demonstrates. A more detailed comparison to the web-based concepts is given in the evaluation in section \ref{sses::eval_compare}.

    \subsection{Common Issues in Modern Development Setups}
    The current congestion of development environments, as described above, is a collection of different programs and their configurations that each developer has to spend worktime on in order to set up a working state suitable to their personal preferences. Only when this has been accounted for the respective project can the developers begin with their actual work. However, their work may be restricted by a chosen software architecture, and interrupted if changes in the code or its runtime cause problems with the development environment. The following section describes the most common issues with local development environments.

        \subsubsection{The Initial Setup Process}\label{sss::initial}
        Newly recruited developers or those changing projects need time and support to settle into the new project. They are not familiar with the code base and instructions on how to set up the environments correctly. Evidently, good code documentation helps as long as it is available, up-to-date and detailed enough. Specific software packages such as \ac{DB}s, interpreters or compilers need to be installed and configured in the correct versions in order to make the local environment operational. Programming languages such as C/C++ and PHP do not included a debugger in their language framework, and they need to be installed and set up separately. Depending on the project, this can be very extensive and require a lot of time and support from other team members. Setting up a platform independent NodeJS project is considerably simpler thanks to the included package manager \wordhighlight{npm} than it is with system specific C++ or PHP projects. A survey by ActiveState shows that more than 25\% of developers need five more hours to set up a development environment. Considering that more than 65\% of all developers do this one to four times a year, 30\% even five to twelve times a year, this adds up to a significant percentage of work hours \cite{setuppain}. A Web-Search also reveals that broad discussions about automating this process actually do exist. Further problems arising from the configuration of the respective programming language are discussed in more detail in the next point. \newline
        Even though some of this initial work can be automated through the use of scripts, these scripts must be created and maintained for each operating system used. They may be a time saver as long as they don't break due to moved download links or unavailable files.

        \subsubsection{Dependency Management \& Configuration Shift}\label{sssec::dependency}
        % Ist etwas vermischt
        Once the development environment is set up, it is a crucial to maintain it in such a way that developers can perform their actual tasks and contribute value to the product. Some programming language frameworks are able to keep dependencies consistent across multiple systems without much effort thanks to their integrated framework tools. NodeJS' current \ac{LTS}-Version 14, for example, installs all required dependencies into the local project folder and keeps a precise record of their versions by means of a \code{package-lock.json} file. Other languages either come without any package manager at all (like C/C++, PHP and Java), or their package manager installs dependencies globally. The existence of tools like Pythons \wordhighlight{venv} package, which creates virtual isolated environments for each project, proves that dependency management is, in fact, a problem in local development environments~\cite{pythonvenv}. Apart from architectural designing, collaborative meetings and testing time, the investigation of bugs and the maintenance of the application setup are the most time-consuming tasks when developers are not coding. Among the problems that can occur, dependency issues are ranked as the third-largest group, just after of package building issues \cite{setuppain}.\newline
        Even the local package installation approach of NodeJS does not solve the problem of developing against multiple versions of the NodeJS framework. Testing a new major version of NodeJS can break existing project configurations, while undoing these changes can be tedious and time-consuming. Additional tools like the \wordhighlight{\ac{NVM}} have to be used in order to run multiple NodeJS versions on the same system. It becomes particularly problematic when developers are working on several projects with different dependency requirements at the same time. Legacy applications may require runtimes and libraries that are no longer supported on modern systems. Versions prior to PHP 7, for example, can no longer be installed (without additional effort) on current Windows or Linux distributions. These issues are assigned to the category of runtime dependency management. In a microservice architecture, the service dependencies further extent this problem. \newline
        In a microservice architecture, there are several granular applications, each providing functionally related logic and having bounded endpoints. Each endpoint can be considered a public \ac{API}, even if the service is only accessible within the overarching service. Communication between these endpoints is called inter-service communication. Maintaining compatibility between applications becomes a challenge, especially with many microservices or when using a service mesh \cite{micro}. If a team member changes the interface of one microservice, this affects other services and the developers responsible for these applications need to be notified and respond in order to avoid unexpected errors. Tools like \wordhighlight{Swagger}, for applying the OpenAPI-Specification, can help cope with these challenges. Nevertheless, the \ac{API} version used must be defined and configured accordingly in other applications. The management of inter-service communication has a direct impact on the testing possibilities, which are discussed in the next section.\newline
        % ToDo
        % Würde terraform zuvor erwähnt und erklärt
        In ideal environments, the local setup would always work as long as no changes are made. However, there are always changes, \ac{OS} and security updates, changes to the dependencies to test a new version or the temporary change of the network port for a side-by-side comparison. The modification of the database schema by one developer can cause a broken environment for other developers. These slight changes over time are called configuration shift. No system is identical to another, which can lead to unreproducible builds and fluky errors. For this reason, the popularity of tools such as \wordhighlight{Terraform}, \wordhighlight{Puppet} and \wordhighlight{Ansible} that implement the \ac{IaC} principle and rely on the automated creation and configuration of \ac{VM}s instead of a manual creation, is increasing in the operational cloud sector. Each instance of a machine is configured in such an exact and consistent way a human being would be incapable of. If a \ac{VM} behaves abnormally, it is torn down and recreated. However, local environments are not considered disposable, they are personalized and therefore hard to create and maintain automatically. Some projects may require exact reproducibility, especially in a scientific context. Yet, this is hard to achieve in indiscriminately configured and personalized environments.\newline
        The tools mentioned above help to keep the system in a consistent, good state, but do not help much with the many changes within the application. Well-known software practices, such as database migrations and test suites, are required to minimize errors within applications, but testing is a particular challenge in a microservice architecture.

        \subsubsection{Lack of Testing Options}\label{sss::testing_problem}
        The goal of tests is to verify the behavior of the system under the given conditions. It is a crucial practice to ensure product quality and high custom value \cite{azuredevops}. Functional tests can be categorized in the four stages shown in Table~\ref{tab::tests}. According to the test pyramid, unit tests are the tests with the smallest scope, but which should be implemented most heavily. They ensure the correct behavior of a function or a class and only relay on the code to be tested itself. Only preceded by a compiler or a linter, they represent the earliest stage able to detect errors. In a \ac{TDD} environment, the tests are even written before the application code itself to ensure that the application meets the project requirements. The use of unit tests remains unchanged in a microservice architecture, as each service can still have its own unit test cases. Due to the number of applications, though, \ac{IPC} in a microservice architecture increases significantly, making integration and higher testing scenarios more complex \cite{microtest}. \newline
        Integration tests should verify that two applications can successfully interact with each other. In order to perform these kinds of tests, it is necessary to run both applications simultaneously \cite{azuredevops}. Triggering an \ac{API} event and verifying its result reveals integration errors, but makes them as complex as end-to-end tests thanks to the configuration and orchestration of multiple services \cite{microtest}. While developers can use tools such as \wordhighlight{Postman} to check the results of an \ac{API} call, this does not guarantee that two applications can actually communicate successfully. Integration tests can either be done in \ac{CI}, where an error can only be detected after the code has already been committed and pushed and \ac{CI} tasks completed, or locally with immense effort for the entire configuration of all services. Due to the high rate of inter process communication, the use of tracing tools may be necessary to isolate an inter-application error. Integrating tracing programs into \ac{CI} and accessing them locally is another significant technical task. This results either in a very late error detection, which slows down development and makes it more expensive, or in an individual, local environment that is difficult to maintain and prone to errors. Both scenarios leave developers with insufficient testing capacities for efficient software development. Although contract testing for compliance with shared \ac{API} specifications can be used for integration testing, this merely postpones the problem to a later test stage \cite{microtest}.\newline
        Entire application tests, called end-to-end tests, are extensive and time-consuming. They require all applications to be deployed to cover entire business logic operations \cite{microtest}. They should be used thoughtful and performed on a testing or staging environment. However, these only form a small part of the test scope and are typically performed by a separate \ac{QA} team. When an error is detected, its cause must be found, for which tracing and debugging programs are used. The absence or previous configuration of these tools complicates the work and is one of the problems already described above. The lack of tools and too complex, differentiated environments are also one of the main challenges in implementing DevOps practices \cite{devops_challenge}.

        \begin{table}[]
            \centering
            \begin{tabularx}{0.9\textwidth}{lX}
                Test-Type & Description \\ \midrule\midrule
                Unit test& Test a small part of a service, such as a class.\\
                Integration tests & Verify that a service can interact with infrastructure services \\
                Component tests & Acceptance tests for an individual service. \\
                End-to-end tests & Acceptance tests for the entire application.
            \end{tabularx}
            \caption{Types of Software Tests, \\\textit{Source:~\cite{microtest}}}\label{tab::tests}
        \end{table}

        \subsubsection{Issues Caused by Heterogeneous Environments}\label{sss::hetero}
        According to the StackOverflow Survey 2021, Windows is the most often used operating system for software development, but in the server area, Unix-based systems clearly dominate with a market share of 75.3\% among webservers \cite{stackoverflow2021}, \cite{unixusage}. The current state of affairs is thus obvious: The development takes place on Windows, whereas the actual operation occurs on Linux. This fundamental difference in the base runtime environment of applications can be the cause of a variety of operating system specific errors, even if the used programming language supports cross-platform compatibility. One major difference is the structure and functioning of the file system. Windows uses alphabetical identifiers like \code{c} and \code{d} for drive partitions. Linux, on the other hand, has a root directory (\code{/}), in which all underlying folders and partitions are placed. External partitions can be mounted at any place and with any name. A frequently used directory for external partitions is to be found in the \code{/mnt} directory. Accordingly, the representation of file paths also differ. While user data under Windows can be accessed via \code{C:\textbackslash Users\textbackslash USERNAME} using backslashes, it is usually accessible via \code{/home/USERNAME} under Linux, with a normal forward-slash character. The inclusion of external libraries, assets and other files via paths is a common task in programming. Hardcoding these paths can lead to unexpected behavior on a different host. Although some programming languages offer constructs such as \code{File.Separator} (Java), or attempt to perform the path-separator conversion automatically (NodeJS and Python), errors still occur on heterogeneous systems. One of the reasons is the special meaning of the backslash, which is often used to escape reserved special characters.\newline
        In a way similar to the file paths, the encoding of the newline character differs. While Windows uses, by default, the \code{CR} line ending format, Linux the \code{LF} format. There are applications, which can read both encodings, but nevertheless, there are also applications that either only support \code{CR} or that can only read newline characters encoded with the \code{LF} format. Bash scripts created under Windows cannot be executed under Linux without a conversion from \code{CR} to \code{LF}. The permission system of Windows and Linux also differs. Windows uses the central uses the central user account control to manage the processes of reading, modifying and changing the owner and to delete permissions. Linux, on the other hand, has a read, write and execute flag every file, determining owner, group and other permissions. Windows does not support the executable flag at all. Accordingly, all files created under Windows, which are then copied to Linux environment, cannot be executed without further steps. Another example for these problems is the handling of keys and certificates. The widely used key-based cryptosystem \ac{RSA} can be used under both Windows and Linux, yet differently. \ac{RSA}-Keys created under Windows are not accepted by many Linux applications because they cannot be read due to \code{CR} line endings or are rejected because \wordhighlight{nginx}, \wordhighlight{apache webserver} and \wordhighlight{sshd} require that private keys are only readable by the user and the permissions are too open by default.\newline
        These heterogeneous based problems occur in addition to \acl{OS} specific programming. Low level operations, such as forking a process, spawning a new one and sending (exit) signals, are fundamentally system-specific. Higher level programming languages abstract some of these operations, yet, some functions are only available on one specific system such as Unix sockets. Dependencies with native libraries are also \acl{OS}-specific. They either have different variants for each system or they are compiled into native libraries on the target system at install time, as to be seen in the cases of NodeJS and \wordhighlight{node-gyp} or Python and \wordhighlight{wheels}, respectively. This deviating behavior adds extra complexity and creates new error sources.

        \subsubsection{Additional Effort for Developers}
        the preceding sections already show how heterogeneous environments, many small services and an extended testing effort cause additional work for developers. In addition to that, there is also the aspect of the management of secrets such as, \ac{API}-tokens or keys, and, for new developers, the setup of \ac{VPN}s and communication tools. Even when the local environment works without errors, there are ongoing obstacles. As mentioned before, the value of microservice is the multiplicity of small applications, which together provide a greater functionality. In order to start multiple microservice developers most likely need multiple terminals to execute each application. A larger amount of terminal sessions can quickly become confusing. Eventually, services even have a specific order for startup. This phenomenon is also referred to as terminal hell. However, there are still obstacles to overcome after the start of the individual applications; tracking their output is also one of the tasks. Log outputs provide insights into what the application is currently doing. This makes it possible to quickly check the expected behavior of an application. The output of many logs on different terminals, however, only contributes to a rapidity increasing confusion. Analogous to the terminals, this is called log hell \cite{micro}.\newline
        These circumstances lead to developers spending more time managing, configuring and analyzing applications rather than actively developing them, which leads to a slower pace of development. Slower development increases cost and leads to customers having to wait longer for new features and bug fixes, which diminishes the customer experience. If a company's business model is to offer software development as a service, slower and higher development costs mean that the company cannot compete with rival companies and loses contracts.

        \subsection{Proposed Solution for more Efficient Workflows}
        The root cause of these problems is that local development environments fundamentally differ from production environments. Local environments are deeply individualized and personalized, while production environments are scandalized but very complex. Local environments cannot be set up automatically and thus cannot simply be torn down and replaced in the event of errors. Long troubleshooting sessions are the consequence resulting in a slow-down of the development progress.\newline
        In the server world, virtualization technology is used for uniform and well-defined environments. A consistent and isolated application runtime environment can be provided by a container based virtualization solution such as Docker. In order to allow interaction between applications, they can be orchestrated and scaled with via a Docker Swarm or Kubernetes cluster.\newline
        This thesis proposes to use exactly this containerization approach for local development environments. Thus, the configuration effort, the lack of testing possibilities and the occurrence of local and operating system specific errors should be reduced.
        % With tools like Terraform or Ansible, any number of server instances can be created in a short time in exactly the same configuration.
