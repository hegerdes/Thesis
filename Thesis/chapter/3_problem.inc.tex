% !TeX root = ../thesis_main.tex

\section{Analysis of Current Development Environments}\label{sec::problem}
The following section will display the state of typical development environments. The most commonly used operating system is compared to productive environments their differences are pointed out. The problems caused by this are described and further challenges in the development work, especially considering a microservice architecture are revealed.

    \subsection{Current State of Development Environments}
    Software development involves a broad variety of tasks. Depending on the project, tasks can range from web development, embedded system development, desktop or mobile application development, data analysis or the pure maintenance of one of these areas. Each of these software development fields has its own workflows and requirements for the actual development setup. Even within these specialized categories, there are different requirements, depending on the scope and size of a project. Accordingly, the setups of development environments can differ greatly from one another. Due to the large scope of different variants, this work will only refer to web services built on a microservice architecture. Nevertheless, this work can also be adapted to other types of software architectures.\newline
    According to StackOverflow Survey 2021, traditional local computers are the most widely used solution for developing software among professional Developer. The primary operating system used is Windows, followed by macOS and Linux \cite{stackoverflow2021}. Accordingly, developers need modern hardware, on which all the required applications must be installed and set up individually. Depending on the application area, this may include some applications for which license costs may have to be paid and which must be kept up to date after the initial installation. The setup and maintenance of these tasks are necessary supporting processes in software development, but they cost a lot of time and effort, and therefore money without producing any actual progress and value to the project. This is especially true if there are many developers in a large company with frequently changing developers. Once the environment is running, developers can start to code and contribute to the project. The developers of \href{https://www.github.com}{GitHub.com} created a series of scripts just to get developers to a working environment in about one day. Because of frequent error these scripts included a global clean option (called \code{-\--nuke-from-orbit}) to reset the environment the initial, known good, state~\cite{githubblogcodespace}.\newline
    Such a local working environment leaves developers in a position, in which they have lots control over their development environment. However, local manual setups require much effort and result in an elaborately error-proneness's environment as the next section demonstrates. A possible solution to these problems is presented in section \ref{sec::solution_concept} and compared with still fairly recently developed alternatives in section \ref{sses::eval_compare}.

    \subsection{Common Issues in Modern Development Setups}
    The current congestion of development environments, as described above, is a collection of different programs and their configurations that each developer has to spend worktime on in order to set up a working state suitable to their personal preferences. Only when this has been accounted for the respective project the developers can begin with their actual work. However, their work may be restricted by a chosen software architecture, and interrupted if changes in the code or its runtime cause problems with the development environment. The following section describes the most common issues with local development environments.

        \subsubsection{The Initial Setup Process}\label{sss::initial}
        Newly recruited developers or those changing projects need time and support to settle into the new project. They are not familiar with the code base and instructions on how to set up the environments correctly. Evidently, good code documentation helps as long as it is available, up-to-date and detailed enough. Specific software packages such as \ac{DB}s, interpreters or compilers need to be installed and configured in the correct versions in order to make the local environment operational. Programming languages such as C/C++ and PHP do not included a debugger in their language framework, and they need to be installed and set up separately. Depending on the project, this can be very extensive and require a lot of time and support from other team members. Setting up a platform independent NodeJS project is considerably simpler as system specific C++ or PHP projects, thanks to the included package manager \wordhighlight{npm}. A survey by ActiveState shows that more than 25\% of developers need five more hours to set up a development environment. Considering that more than 65\% of all developers do this one to four times a year, 30\% even five to twelve times a year, this adds up to a significant percentage of work hours \cite{setuppain}. A Web-Search also reveals that broad discussions about automating this process actually do exist. The topic seems to be mostly practice oriented and scientific papers on the topic are unfortunately rare. Even though some of this initial work can be automated through the use of scripts, like the ones used by GitHub, these scripts must be created and maintained for each operating system used. They may be a time saver as long as they don't break due to moved download links or unavailable files.\newline
        Further problems arising from the configuration of the respective programming language are discussed in more detail in the next point.

        \subsubsection{Lack of Testing Options}\label{sss::testing_problem}
        The goal of tests is to verify the behavior of the system under the given conditions. It is a crucial practice to ensure product quality and high custom value \cite{azuredevops}. Functional tests can be categorized in the four stages shown in Table~\ref{tab::tests}. According to the test pyramid, unit tests are the tests with the smallest scope which, however, should be implemented most heavily. They ensure the correct behavior of a function or a class and only depend on the code to be tested itself. Only preceded by a compiler or a linter, they represent the earliest stage to detect errors. In a \ac{TDD} environment, the tests are even written before the application code itself to ensure that the application meets the project requirements. The use of unit tests remains unchanged in a microservice architecture, as each service can still have its own unit test suite. Due to the number of applications, though, \ac{IPC} in a microservice architecture increases significantly, making integration and higher testing scenarios more complex \cite{microtest}. \newline
        Integration tests should verify that multiple applications can successfully communicate with each other. In order to perform these kinds of tests, it is necessary to run both applications simultaneously \cite{azuredevops}. While developers can use tools such as \wordhighlight{Postman} to check the results of a \ac{API} request, this does not guarantee that two applications can actually communicate successfully. Integration tests can either be performed in a \ac{CI} system, where an error can only be detected after the code has already been pushed the central repository and all \ac{CI} tasks are completed, or locally with immense configuration effort. Due to the high rate of interprocess communication, the use of tracing tools may be necessary to isolate any inter-application errors. Integrating them individually into local developer setups is another tedious task that does not add any direct value to the project. Besides software design and meetings, explicit testing of applications consumes most of a programmer's time when he is not coding \cite{setuppain}.\newline
        The result of both integration testing variants is either in a very late error detection, which slows down development and makes it more expensive, or in an individual, local environment that is difficult to maintain and prone to errors, as the next section will show. Both scenarios leave developers with insufficient testing capacities for efficient software development. Although contract testing for compliance of \ac{API} specifications can be used for integration testing, this merely postpones the problem to a later test stage \cite{microtest}.\newline
        Entire application tests, called end-to-end tests, are extensive and time-consuming. They require all applications to be deployed in order to cover entire business logic operations \cite{microtest}. They should be used thoughtful and performed on a testing or staging environment. However, these only form a small part of the test scope and are typically performed by a separate \ac{QA} team. When an error is detected, its cause must be found, for which tracing and debugging programs are used. The absence or tedious configuration of these tools complicates identifying the bug and is one of the problems already described above. The lack of tools and too complex, divagating development environments are another main challenges for implementing DevOps practices \cite{devops_challenge}.

        \begin{table}
            \centering
            \begin{tabularx}{0.9\textwidth}{lX}
                Test-Type & Description \\ \midrule\midrule
                Unit test & Test a small part of a service, such as a class.\\
                Integration tests & Verify that a service can interact with infrastructure services \\
                Component tests & Acceptance tests for an individual service. \\
                End-to-end tests & Acceptance tests for the entire application.
            \end{tabularx}
            \caption{Stages of Software Tests, \\\textit{Source:~\cite{microtest}}}\label{tab::tests}
        \end{table}

        \subsubsection{Configuration and Dependency Management}\label{sssec::dependency}
        Once the development environment is set up, it is a crucial to maintain it in such a way that developers can perform their actual tasks and contribute value to the product. This includes keeping both configuration and dependencies in a good, consistent state with colleagues and the productive environment. Some programming language frameworks are able to keep dependencies consistent across multiple systems without much effort thanks to their integrated tools. The current NodeJS 14 \ac{LTS}-Version, for example, installs all required dependencies into the local project folder and keeps a precise record of their versions by means of a \code{package-lock.json} file. Other languages either come without any package manager at all (like C/C++, PHP and Java), or their package manager installs dependencies globally (Go, Python). The existence of tools like Pythons \wordhighlight{venv} package, which creates virtual isolated dependency environments for each project, proves that dependency management is, in fact, a problem in local development environments~\cite{pythonvenv}. Apart from architectural designing, collaborative meetings and testing time, the investigation of bugs and the maintenance of the application setups are the most time-consuming tasks for developers are not coding. Among the bugs that can occur in a project, dependency issues are ranked as the third-largest group, just after of packaging issues \cite{setuppain}.\newline
        Even the local package installation approach of NodeJS does not solve the problem of developing against multiple versions of the NodeJS framework. Testing a new major version of NodeJS can break existing project configurations, undoing these changes can be tedious and time-consuming. Additional tools like the \wordhighlight{\ac{NVM}} have to be used in order to run multiple NodeJS versions on the same system. It becomes particularly problematic when developers are working on several projects with different dependency requirements at the same time. This also includes the maintenance of legacy applications. Legacy applications may require runtimes and libraries that are no longer supported on modern systems. Versions prior to PHP 7, for example, can no longer be installed (without additional effort) on current Windows or Linux distributions. These issues are assigned to the category of runtime dependency management. In a microservice architecture, the service dependencies between different applications further extent this problem. \newline
        In a microservice architecture, there are several granular applications, each providing functionally related logic and having bounded endpoints. Each endpoint can be considered a public \ac{API}, even if the service is only accessible within the overarching service. Communication between these endpoints is called inter-service communication. Maintaining compatibility between applications becomes a challenge, especially with many microservices or when using a service mesh \cite{micro}. If a team member changes the interface of one microservice, this affects other services and the developers responsible for these applications need to be notified and respond in order to avoid unexpected errors. This problem is particularly evident in the microservice Death-Star, which shows all of Netflix's microservices as part of a confusing point cloud with numerous connections to each other \cite{deathstar}. Every single compatible \ac{API} endpoint must be defined and configured in every other application that uses it. The management of these inter-service communication settings has a direct impact on the testing possibilities, which are discussed in the previous section. This illustrates once again how much effort local testing of applications can have.\newline
        In ideal environments, the local setup would always work as long as no changes are made. However, there are always changes, \ac{OS} and security updates, changes to the dependencies or the temporary change of the network port for a side-by-side comparison. The modification of the database schema by one developer can cause a broken environment for other developers. These slight changes over time are called configuration shift. No system is identical to another, which can lead to unreproducible builds and fluky errors. To avoid such errors in production, tools like \wordhighlight{Terraform}, and \wordhighlight{Ansible} are used. These tools implement the \ac{IaC} priceable and can create multiple server configurations that are exactly identical to the state defined in a playbook or config file. Each instance of a machine is configured in such an exact and consistent way a human being would be incapable of. If a \ac{VM} behaves abnormally, it is torn down and recreated. However, local environments are not considered disposable, they are personalized and therefore hard to create and maintain automatically. Some projects may even require perfect reproducibility, especially in a scientific context. Yet, this is hard to achieve in indiscriminately configured and personalized environments.\newline
        Managing different project configurations that conform to team members and the production environment thus takes considerable time and, according to ActiveState's study and GitHub's experience, is also responsible for a considerable number of errors that occur during development. Examples of the errors are further elaborated in the next section \cite{setuppain}, \cite{githubblogcodespace}.

        \subsubsection{Issues Caused by Heterogeneous Environments}\label{sss::hetero}
        According to the StackOverflow Survey 2021, Windows is the most widely used operating system for software development, but in the server area, Unix-based systems clearly dominate with a market share with a percentage 75.3\% among webservers \cite{stackoverflow2021}, \cite{unixusage}.
        In data-centers 80\% of workload is virtualized, the technology primarily used for this is based on Linux. The current state of affairs is thus obvious: The development takes place on Windows, whereas the actual operation occurs on Linux. This fundamental difference in the core runtime environment of applications can be the cause of a variety of operating system specific errors, even if the used programming language supports cross-platform compatibility. One major difference is the structure and functioning of the file system. Windows uses alphabetical identifiers like \code{c} and \code{d} for drive partitions. Linux, on the other hand, has a root directory (\code{/}), in which all underlying folders and partitions are located. External partitions can be mounted at any place and with any name. A frequently used directory for external partitions is to be found in the \code{/mnt} directory. Accordingly, the representation of file paths also differ. While user-data on Windows is located in \code{C:\textbackslash Users\textbackslash USERNAME} using backslashes, On Linux it is usually accessible via \code{/home/USERNAME}, with a normal forward-slash character. The inclusion of external libraries, assets and other files via paths is a common task in programming. Hardcoding these paths can lead to unexpected behavior on a different host. Although some programming languages offer constructs such as \code{File.Separator} (Java), or attempt to perform the path-separator conversion automatically (NodeJS and Python), errors still occur on heterogeneous systems. Prominent problematic examples of this are the scripting languages Bash and PowerShell, which cannot handle alternative path separators. One of the reasons is the special meaning of the backslash, which is often used to escape reserved special characters.\newline
        In a way similar to the file paths, the encoding of the newline character differs depending on the \ac{OS}. While Windows uses, by default, the \code{CR} line ending format, Linux uses the \code{LF} format. There are applications, which can read both encodings, but nevertheless, there are also applications that either only support \code{CR} or that can only read newline characters encoded with the \code{LF} format. Bash scripts created on Windows cannot be executed under Linux without a conversion from \code{CR} to \code{LF}. The permission system of Windows and Linux also differs. Windows uses the central user account control in order to manage the processes of reading, modifying and changing the owner of a file. Linux, on the other hand, has a read, write and execute flags for every file, determining owner, group and other permissions. Windows does not support the executable flag at all. Accordingly, all files created on Windows, which are then copied to a Linux environment, cannot be executed without further steps. Another example for these problems is the handling of keys and certificates. The widely used key-based cryptosystem \ac{RSA} can be used under both Windows and Linux, yet differently. \ac{RSA}-Keys created under Windows are not accepted by many Linux applications because they are not readable due to \code{CR} encoded line endings or are rejected because \wordhighlight{nginx}, \wordhighlight{apache webserver} and \wordhighlight{sshd} require that private keys are only readable by the user and the permissions are too open by default.\newline
        These heterogeneous based problems occur in addition to \acl{OS} specific programming. Low level operations, such as forking a process, spawning a new one and sending (exit) signals, are fundamentally system-specific. Higher level programming languages abstract some of these operations, yet, some functions are only available on one specific system such as Unix sockets. Dependencies with native libraries are also \acl{OS}-specific. Either there has to be different variant for each \ac{OS} and each system architecture, or the library has be compiled into native library on the target system at install time. For example, NodeJS uses the \wordhighlight{node-gyp} module and Python uses \wordhighlight{wheels} in order to compile native liabilities on the host when it is needed. Native libraries are often included for cryptographic tasks in particular, since these are especially efficient. The uncertainty whether all the necessary tools are available on each system and have the correct version where these libraries are required adds extra complexity and creates new error sources. These potential differences are valid for the development environments between developers within a team as well as in comparison to the productive environment.

        \subsubsection{Additional Effort for Developers}
        The preceding sections already showed how heterogeneous environments, many small services and an extended testing effort are causing additional work for developers. In addition to that, there is also the management of secrets such as, \ac{API}-tokens or keys, and, for new developers, the setup of \ac{VPN}s and communication tools. Even when the local environment works without errors, there are ongoing obstacles. As mentioned before, the value of microservice is the multiplicity of small applications, which together provide a greater functionality. In order to start multiple microservice developers most likely need multiple terminals to execute each application individually. A larger amount of terminal sessions can quickly become confusing. Eventually, services even have a specific order for startup. This phenomenon is also referred to as terminal hell. However, there are still obstacles to overcome after the start of the individual applications; tracking their output is also one of the tasks. Log outputs provide insights into what the application is currently doing. This makes it possible to quickly check the expected behavior of an application. The output of many logs on different terminals, however, only contributes to a rapidity increasing confusion. Analogous to the terminals, this is called log hell \cite{micro}.\newline
        These circumstances lead to developers spending more time managing, configuring and analyzing applications rather than actively developing them, which leads to a slower pace of development. Slower development increases cost and leads to customers having to wait longer for new features and bug fixes, which diminishes the customer experience. If a company's business model is to offer software development as a service, slower and higher development costs mean that the company cannot compete with rival companies and loses contracts.

        \subsection{Proposed Solution for more Efficient Workflows}
        The root cause of these problems is that local development environments fundamentally differ from production environments. Local environments are heavily individualized and personalized, while production environments are standardized but very complex. Local environments cannot be set up automatically and thus cannot simply be torn down and replaced in the event of errors. Long troubleshooting sessions are the consequence resulting in a slow-down of the development progress.\newline
        In the server world, virtualization technology is used in order to archive uniform and well-defined environments. A consistent and isolated application runtime environment is provided by a container based virtualization solution such as Docker. In order to allow interaction between applications, they can be orchestrated and scaled with via a Docker Swarm or a Kubernetes cluster.\newline
        This thesis proposes to use exactly this containerization approach for local development environments. Thus, the configuration effort, the lack of testing possibilities and the occurrence of local and operating system specific errors should be reduced.
        \newpage
