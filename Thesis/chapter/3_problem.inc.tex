% !TeX root = ../thesis_main.tex


\section{Analysis of the Current State of Development Environments}\label{sec::problem}
The following section will display the current status of a typical development environment, point out recurrent problems and provide approaches for possible solutions.

    \subsection{Current State of Development Environments}
    Software development involves a broad variety of tasks. Depending on the project, tasks can range from web development, embedded system development, desktop or mobile application development, data analysis or the pure maintenance of one of these areas. Each of these software development fields has its own workflows and requirements for the actual development setup. Even within these specialized categories, there are different requirements, depending on the scope, size, and general demand for capabilities and power of a project. Accordingly, the setups of development environments can differ greatly from one another. Due to the large scope of the subject and in order to set useful limitations, this work will only refer to web services built on a microservice architecture. Nevertheless, parts of this work can also be adapted to other types of development.\newline
    Despite the increasing use of web-based \ac{IDE}s, such as \wordhighlight{CodeSandbox}, \wordhighlight{StackBlitz}, \wordhighlight{Codespaces} and \wordhighlight{Gitpod}, native development environments are still used primarily. The StackOverflow Survey 2021 supports this thesis and clarifies further that Windows is the primarily used operating system, among professional developers, followed by macOS and Linux~\cite{stackoverflow2021}. Accordingly, developers need modern hardware, on which all the required applications must be installed and set up locally. Depending on the application area, this may include some applications for which license costs may have to be paid and which must be kept up to date after the initial installation. The setup and maintenance of these tasks are necessary supporting processes in software development, but they can cost a lot of time and effort, and therefore money without producing any actual progress and value. This is especially true if there are many developers in a large company with frequently changing developers. Once the environment is running, developers can start to code and contribute to the project. Programming is about changing things and changes can break things, as can be seen in the case of GitHub.com. Here, developers created a series of scripts just to get developers to a working environment in less than a day. Because of frequent error these scripts included a global clean option (called \code{--nuke-from-orbit}) to reset the environment the initial, known good, state~\cite{githubblogcodespace}.\newline
    Such a local working environment leaves developers in a position, in which they have more control over their development environment than in the case of the pre-defined and functionally restricted web-based environments.
    However, local setups require much effort and are result in an elaborately error-proneness's environment as the next section demonstrates. A more detailed comparison to the web-based concepts is given in the evaluation in Section \ref{sses::eval_compare}.

    \subsection{Common Issues in Modern Development Setups}
    The current congestion of development environments, as described above, is a collection of different programs and their configurations that each developer has to spent worktime on setting it up to a working state that suits their personal preferences. Only when this has been set up for the respective project developers can begin with their actual work. However, their work may be restricted by the chosen software architecture and interrupted if changes in the code or its runtime cause problems with the development environment. The following section describes the most common issues with local development environments.

        \subsubsection{The Initial Setup Process}\label{sss::initial}
        Newly recruited developers or those changing projects need time and support to settle into the new project. They are not familiar with the code base and instructions on how to set up the environments correctly. Of course, good code documentation helps, as long as it is available, up-to-date and detailed enough. Specific software packages such as \ac{DB}s, interpreters or compilers need to be installed and configured in the correct versions to make the local environment operational. Often debuggers are not included in the language framework and do not need to be installed and set up additionally. Depending on the project, this can be very extensive and require a lot of time and support from other team members. Setting up a platform independent NodeJS project is considerably simpler thanks to the included package manager \wordhighlight{npm} than it is with C++ or PHP projects. A survey by ActiveState showed that over 25\% of developers need five more hours to set up a development environment. Considering that over 65\% of developers do this one to four times a year, 30\% even five to twelve times a year, this adds up to a significant percentage of work hours \cite{setuppain}. A Web-Search also reveals a wide range of discussions about automating this process exist. Further problems that arise from the configuration of the respective programming language are discussed in more detail in the next point. \newline
        Even though some of this initial work can be automated with scripts, these scripts must be created and maintained for each operating system used. These can be a time saver as long as they don't break due to moved download links or unavailable files.

        \subsubsection{Dependency Management \& Configuration Shift}\label{sssec::dependency}
        Once the development environment is set up it is a crucial job to maintain it in such a way that developers can perform their actual tasks and contribute value to the product. Some programming languages frameworks are able to keep dependencies consistent across multiple systems without much effort thanks to their integrated framework tools that come with them. NodeJS current \ac{LTS}-Version 14 for example installs all the required dependencies locally in the project folder and keeps a precise record of their versions by means of a \code{package-lock.json} file. Other languages either come without any package manager at all (like C/C++, PHP and Java) or their package manager installs dependencies globally. The existence of tools like Pythons \wordhighlight{venv} package, for creating virtual isolated environments for each project, proof that dependency management is in fact a problem in local development environments~\cite{pythonvenv}. Apart from architectural designing, meetings and testing investigating bugs and maintaining the application setup are the tasks that consume the most amout of time when developers are not coding. Among the problems that occur, dependency issues are ranked as the third-largest group, just after of package building issues \cite{setuppain}.\newline
        Even the local package installation approach of NodeJS do not solve the problem of developing against multiple versions of the NodeJS framework. Testing a new major version of NodeJS can break existing project configurations. Undoing these changes can be tedious and time-consuming. Additional tools like the \wordhighlight{\ac{NVM}} have to be used in order to run multiple NodeJS versions on the same system. It becomes particularly problematic when developers are working on several projects at the same time and these have different dependency requirements. Legacy applications may require runtimes and libraries that are no longer supported on modern systems. Versions prior to PHP 7, for example, can no longer be installed (without extra effort) on current Windows or Linux distributions. These issues are assigned into the runtime dependency management category. In a microservice architecture, the service dependencies further extended this problem. \newline
        In a microservice architecture, there are several granular applications, each of which provide functionally related logic and has bounded endpoints. Each endpoint can be considered to be a public \ac{API}, even if the service is just accessible within the overarching service. Communication between these endpoints is called inter-service communication. Maintaining compatibility between applications becomes a challenge especially with many microservices or when using a service mesh \cite{micro}. If a team member changes the interface of one microservice it will affect other services and the developers responsible for these applications need to be notified and respond in order to avoid unexpected errors. Tools like \wordhighlight{Swagger} for applying the OpenAPI-Specification can help with these challenges. Nevertheless, the \ac{API} version that is used must be defined and configured accordingly in other applications. The management of inter-service communication has a direct impact on the testing possibilities, which are discussed in the next section.\newline
        In ideal environments, the local setup would always work as long as no changes are made. But there are always changes, \ac{OS} and security updates, changes to the dependencies to test a new version or the temporary change of the network port for a side by side comparison. The modification of the database schema by one developer can cause a broken environment for other developers. These slide changes over time are called configuration shift. No system is identical to another which can lead to unreproducible builds and fluky errors. For this reason, the popularity of tools such as \wordhighlight{Terraform}, \wordhighlight{Puppet} and \wordhighlight{Ansible} that implement the \ac{IaC} principle and rely on the automated creation and configuration of \ac{VM}s instead of manual creation is increasing in the operational cloud sector. Each instance of a machine is configured in an exact consistent way a human being could not do. If a \ac{VM} behaves abnormally, it is tern down and recreated. However, local environments are not considered disposable, they are personalized and therefor hard to automatically create and maintain. Some projects may require exact reproducibility, especially in a scientific context. This is difficult to achieve in indiscriminately configured and personalized environments.\newline
        The tools mentioned above help to keep the system in a consistent good state, but do not help much with the many changes within the application. Well-known software practices such as database migrations and test suites are required to minimize errors within applications, but testing is a particular challenge in a microservice architecture.

        \subsubsection{Increased Testing Effort}\label{sss::testing_problem}
        The goal of tests is to verify the behavior of the system under the given conditions. It is a crucial practice to ensure product quality and high custom value \cite{azuredevops}. Functional tests can be categorized into the four stage shown in Table~\ref{tab::tests}. According to the test pyramid, unit tests are the tests with the smallest scope, but which should be implemented most heavily. They ensure the correct behavior of a function or a class and only relay on the code to be tested itself. After the compiler or linter, they are the erliest stage to detect errors. In a \ac{TDD} environment the tests are even written before the application code itself to ensure the application meets the project requirements. The use of unit tests remains unchanged in a microservice architecture as each service can still have its own unit test cases. Due to the number of applications, though, \ac{IPC} in a microservice architecture increases significantly, making integration and higher testing scenarios more complex \cite{microtest}. \newline
        Integration tests should verify that two applications can successfully interact with each other. In order to perform these it is necessary to is to run both applications simultaneously \cite{azuredevops}. Triggering an \ac{API} event and verifying its result shows integration errors, but makes them as complex as end-to-end tests thanks to the configuration and orchestration of multiple services \cite{microtest}. While developers can use tools such as \wordhighlight{Postman} to check the results of an \ac{API} call, this does not guarantee that two applications can actually communicate successfully. Integration tests can either be done in \ac{CI} where an error can only be detected after code has already been committed, pushed and \ac{CI} tasks completed, or locally with immense effort for the entire configuration of all services. Due to the high rate of inter process communication, the use of tracing tools may be necessary to isolate an inter application error. Integrating tracing programes into \ac{CI} and accessing them locally is another significant technical task. This results either in very late error detection, which slows down development and makes it more expensive, or in an individual, local environment that is difficult to maintain and prone to errors. Both scenarios leave developers with insufficient testing capacities for efficient software development. Although contract testing for compliance with shared \ac{API} specifications can be used for integration testing, this merely postpones the problem to a later test stage \cite{microtest}.\newline
        Entire application tests, called end-to-end tests, are extensive and slow. They need to have all applications to be deployed to cover entire business logic operations \cite{microtest}. They should be used thoughtful and performed on a testing or staging environment. However, these only form a small part of the test scope and are typically performed by a separate \ac{QA} team. When an error is detected, its cause must be found, for which tracing and debugging programs are used. The absence or previous configuration of these tools complicates the work and is one of the problems already described above. The lack of tools and too complex, differentiated environments are also one of the main challenges in implementing DevOps practices \cite{devops_challenge}.

        \begin{table}[]
            \centering
            \begin{tabularx}{0.9\textwidth}{lX}
                Test-Type & Description \\ \midrule\midrule
                Unit test& Test a small part of a service, such as a class.\\
                Integration tests & Verify that a service can interact with infrastructure services \\
                Component tests & Acceptance tests for an individual service. \\
                End-to-end tests & Acceptance tests for the entire application.
            \end{tabularx}
            \caption{Different types of software tests, \\\textit{Source:~\cite{microtest}}}\label{tab::tests}
        \end{table}

        \subsubsection{Issues Caused by Heterogeneous Development Environments}\label{sss::hetero}
        According to the StackOverflow Survey 2021, Windows is the most often used operating system for software development, but in the server area, Unix-based systems clearly dominate with market share of 75.3\% among webservers \cite{stackoverflow2021}, \cite{unixusage}. The current state of affairs is thus obvious: Develop on Windows operate on Linux. This fundamental difference in the base runtime environment of applications can be the cause of a variety of operating system specific errors, even if the used programming languages supports cross-platform compatibility. One major differences is the structure and functioning of the file system. Windows uses alphabetical identifiers like \code{c} and \code{d} for drive partitions. Linux, on the other hand, has a root directory (\code{/}) in which all underlying folders and partitions are placed. External partitions can be mounted at any place and with any name. A frequently used directory for external partitions is under the \code{/mnt} directory. Accordingly, the representation of file paths also differ. While user data under Windows can be accessed in \code{C:\textbackslash Users\textbackslash USERNAME} using backslashes, under Linux it is usually \code{/home/USERNAME} with a normal slash character. The inclusion of external libraries, assets and other files via paths is a common task in programming. Hardcoding these paths can lead to unexpected behavior on a different host. Although some programming languages offer constructs such as \code{File.Separator} (Java) or attempt to perform the path-separator conversion automatically (NodeJS and Python), errors still occur on heterogeneous systems. One of the reasons is the special meaning of the backslash, which is often used to escape reserved special characters.\newline
        Similar to the file paths, the encoding of the newline character differs. Windows uses the \code{CR} line ending format and Linux the \code{LF} format. There are applications that can read both encodings nevertheless, there are also applications that either only support \code{CR} or can only read LF encoded newline characters. Bash scripts created under Windows cannot be executed under Linux without a conversion from \code{CR} to \code{LF}. The permission system of Windows and Linux also differ. Windows uses the central user account control for managing read, modify, change owner and delete permissions. Linux, on the other hand, has a read, write and execute flag for the owner, group members and other users for every file. Windows does not support executable flag at all. Accordingly, all files created under Windows which are then copied to Linux environment cannot be executed without further steps. Another example for these problems is the handling of keys and certificates. The widely used key-based cryptosystem \ac{RSA} can be used under both Windows and Linux, but with differences. \ac{RSA}-Keys created under windows are not accepted by many Linux applications because they cannot be read due to \code{CR} line endings or are rejected because \wordhighlight{nginx}, \wordhighlight{apache webserver} and \wordhighlight{sshd} require that private keys are only readable by the user and the permissions are too open by default.\newline
        These heterogeneous based problems come in addition to \acl{OS} specific programming. Low level operations such as forking a process, spawning a new one and sending (exit) signals are fundamentally system specific. Higher level programming languages abstract some of these operations, yet some functions are only available on one specific system such as Unix sockets. Dependencies using native lieberies are also \ac{OS} specific. They either have different variants for each host or they are compiled into native libraries on the traget system at install time like NodeJS does with \wordhighlight{node-gyp} or Python with \wordhighlight{wheels}. Having this deviating behavior adds extra complexity and creates new error sources.

        \subsubsection{Additional Effort for Developers and extra Cost}
        The above sections already show how heterogeneous environments, many small services and an extended testing effort cause additional work for developers. In addition, there is also the management of secrets such as \ac{API}-tokens or keys and, for new developers, the setup of \ac{VPN}s and communication tools. Even when the local environment runs there are ongoing obstacles. As said before the value of microservices is to have multiple small application that together provide a greater functionality. To start multiple microservice developers most likely need multiple terminals to execute each application. Having three or more terminal sessions can quickly become confusing. Eventually services even have a specific order for startup. This phenomenon is also referred to as terminal hell. However, the obstacles do not stop at the start of the individual applications; tracking their output is also one of the tasks. Log outputs provide insights into what the application is currently doing. This makes it possible to quickly check the expected behavior of an application. The output of many logs on different terminals, however, only contributes to a rapidity increasing confusion. Analogous to the terminals, this is called log hell \cite{micro}.\newline
        These circumstances lead to developers spending more time managing, configuring and analyzing applications rather than actively developing them. Which leads to a slower pace of development. Slower development increases cost and leads to customers having to wait longer for new features and bug fixes, which diminishes the customer experience. If a company's business model is to offer software development as a service, slower and higher development costs mean that the company cannot compete with rival companies and loses contracts.

        \subsection{Proposed Solution}
        The root cause of these problems is that local development environments fundamentally differ from production environments. Local environments are deeply individualized and personalized, while production environments are scandalized but very complex. Local environments cannot be set up automatically and thus cannot simply be torn down and replaced in the event of errors. Long troubleshooting sessions are the consequence resulting in a slow-down of the development progress.\newline
        In the server world, virtualization technology is used for uniform and well-defined environments. With tools like Terraform or Ansible, any number of server instances can be created in a short time in exactly the same configuration. A constant and isolated application runtime environment is provided by a virtualization solution such as Docker and applications are orchestrated and scaled with a Docker Swarm or Kubernetes cluster.\newline
        This thesis proposes to use exactly this virtualization approach for local development environments. Thus, the configuration effort, the lack of testing possibilities and the occurrence of local and operating system specific errors should be reduced.
