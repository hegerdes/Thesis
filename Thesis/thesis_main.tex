% !TeX TXS-program:compile = txs:///pdflatex/[--shell-escape]
\documentclass[12pt, a4paper]{article}

\include{inc/packages.inc}
\include{inc/personalize.inc}
\include{inc/style.inc}
\include{inc/listings-style.inc}

% Page style: plain or fancy pans
\pagestyle{fancy}
\fancyhf{}
\fancyhfoffset[L]{1cm} % left extra length
\fancyhfoffset[R]{1cm} % right extra length
\rhead{\thepage}
\lhead{\nouppercase\leftmark}
\cfoot{\fancyplain{}{\thepage} }
% \pagestyle{plain}

% DOC START
\begin{document}

\pagenumbering{gobble}
\include{inc/title.inc}
% For printing add this
% \newpage\null\thispagestyle{empty}\newpage

% Title & Abstract
\maketitle
\begin{abstract}
    \textbf{English:} \lipsum[20]
\end{abstract}
\begin{abstract}
    \textbf{German:} \lipsum[20]
\end{abstract}
\newpage

% Table of Contents
\tableofcontents
\newpage

% Normal page numbering
\newcounter{lastroman}
\setcounter{lastroman}{\value{page}}
\pagenumbering{arabic}

% Line spacing
% \onehalfspacing{}

\section{Introduction}\label{sec::intro}
Digital service solutions are becoming more and more relent and popular as a result of the ever-increasing possibilities and availability of technology. Due to the COVID-19 pandemic new remote working tools, digital education resources, a connected healthcare system and seamless (video) communication systems are needed. The ecommerce marked grew by 32\% up to sales volume of \$791.70 \cite{online_shopping_inc} and the overall revenue in the software market ist expected to grow from 532 billions (2020) to 772 Billion in 2025 \cite{software_industry_groth}. The need for digital business solution is bigger than ever before. To provide fast and suitable software solutions developers need adequate development setups, called development environments. This thesis analysis modern agile software development environments, points out potential problems and purposes a virtualized software development solution to improve development efficiency.\newline
This Section provides an overview about this thesis. It will briefly describe the fundamental core question of this thesis, point out its relevance and presents a possible solution approach. Subsequently, a clear delineation is then given as to what is and is not covered in this thesis. The last Section of the introduction gives a structural overview of how the thesis is structured and how it will approach the given problem.

    \subsection{Structural Overview}
    This Section gives an initial overview and delineation to this thesis. Following this Section, there is a description of the main topic of this work and the goal that should be achieved. Section \ref{sec::backgrund} provides overall background information, which are recommended for further understanding. Common and modern development methodologies are described in Section \ref{ssec::devops}, followed by Microservice and container basics in Section \ref{ssec::microservices} and \acl{CI} tools in \ref{ssec::ci_cd}. In Section \ref{sec::problem} will be a detailed analysis of current development setups and the problems encountered in these setups. The problems are illustrated with examples and their consequences for the development process, the overall project and its quality, which is emblematic of user satisfaction, are presented. Section \ref{sec::solution_concept} proposes a conceptual solution, defines its usability and workings while also providing conditions and limitations for such a solution. This solution concept is applied in Section \ref{sec::solution_code} by implementing it in a practical reference project. The implementation process is described and additional challenges and tricks for practical use are presented. Section \ref{sec::eval} discusses the proposed solution and shows its strengths and weaknesses. At the end, an outlook on future application areas and alternative solutions is given in Section \ref{sec::outlook}, followed by the conclusion in Section \ref{sec::conclusion}.

    \subsection{Problem Description}
    The rise of Agile development and Microservices was accelerated by the emergence of new technologies that changed the way \textit{where} and \textit{how} software is running. Applications could be distributed and scaled quickly through the cloud which benefited customers favors for quick changes. Yet the actual coding setup has not changed significantly.\newline
    Requirements for development environments differ depending on the project type. The development process of (\acs{GUI}) applications for PC's and smartphones is quite different to web-services development. As the functionality of web services continues to grow, while being a cost-effective way to deliver across platforms products, this method is becoming increasingly popular. However, the operating system used for the development process often differs to the operating system used to run these applications in production. This can cause platform dependent errors. Managing language runtime versions between team members or different projects become a challenge, as these can lead to unexpected program behavior or result in library version conflicts. The usage of a \acl{MSA}, due to its easy scalability, adaptability and rapid development, brings further problems. Microservices require additional configuration effort and increases the difficulty level for whole system tests, called end-to-end tests. The initial setup for new developers can get quiet complex, requires time and might even discourage developers in open source projects.\newline
    These characteristics add additional effort, can introduce new errors and slow down the development speed, resulting in higher cost and lower customer experience.

    \subsection{Scope of the Thesis}
    The range of different software development environments is large and differs significantly from each other depending on the project. Since the development of web-based solutions is becoming more and more popular, priority is given only to these types of projects. Native application development for PCs and smartphones, as well as the development of embedded systems and other hardware-related solutions are not covered in this thesis.\newline
    In particular, the solution concept shown is not a general solution that is a perfect fit for all projects and therefore is not generally transferable. It is intended only as a solution template that contains many tricks for dealing with the problems described in Section \ref{sec::problem}. Although Section \ref{sec::backgrund} offers an explanation of fundamental topics and Section \ref{ssec::toolsused} a deeper insight into the functionality of technologies used, nevertheless not all addressed contents can be explained fundamentally. Basic knowledge of the software development process, essential programs and abstract understanding of different operating systems and network techniques are recommended.

    \subsection{Goal of the Theses}
    In the process of this work, the challenges and obstacles of modern software development environments are identified and presented. Based on these findings, a solution concept, for these challenges, is created based on virtualization technologies. The technologies used are explained, and the solution concept is applied to a reference project. Subsequently, the practicability of the solution approach is evaluated and classified. In principle, the goal of this thesis is to identify obstacles in current software development setups and to analyze the effectiveness of the proposed solution.\newpage

\section{Background Information about Agile, Container and Microservices}\label{sec::backgrund}
The following Section provides some basic definitions and principles that are necessary for the further understanding of this paper and ensures a common level of knowledge of these topics.\newline
First of will be a formal definition of DevOps, a brief description of the emergence and its current state.

    \subsection{Modern Methodologies: Agile and DevOps}\label{ssec::devops}
    This Section provides a formal definition of agile software development Paradigms and the DevOps methodology, explains their connection and points out the fundamental principles of a DevOps enabled culture. These principles and their workflows will be revisited in Section~\ref{sec::solution_concept} as part of the proposed solution concept.

        \subsubsection{Agile Development}
        Agile, just like the \wordhighlight{V-Model}, \wordhighlight{Waterfall} and \wordhighlight{Prototyping} model, is a software development paradigm. It was proposed and popularized by the \wordhighlight{"Manifesto for Agile Software Development"}, written and published in 2001 by various authors~\cite{manifesto}. Rapid adoption to changes, continuous evolution of software and customer communication are the fundamentals of agile software development.\newline
        The four core principles are:

        \begin{itemize}[label=\(\star\)]
            \setlength\itemsep{0em}
            \item \textbf{Individuals and interactions} over processes and tools
            \item \textbf{Working software} over comprehensive documentation
            \item \textbf{Customer collaboration} over contract negotiation
            \item \textbf{Responding to change} over following a plan
        \end{itemize}

        \noindent Paradigms like Waterfall describe a comprehensive model with a detailed requirements analysis and architecture design phase. This results in a fixed and detailed sequential schedule for the implementation, testing and requirements fulfillment verification phase. Errors in the requirements analysis or changes in requirements can cause difficulties later phases of the project. Agile on the other handle only provides guidelines instead of a complete model. Changes are expected, and the project realization is designed to be adaptable. Project phases such as implementation and testing run together and the entire \ac{SDLC} is kept shorter and more adaptive \cite{agile_practice}.\newline
        Agile provides a mindset for projects with uncertain or continuous changing requirements. Accordingly, the agile manifesto became the foundation of several software development mythologies and frameworks that extend these fundamental guidelines and provide additional workflows and tooling for the software creation process. Well known examples are methodologies such as \wordhighlight{SCRUM}, \wordhighlight{\ac*{XP}} and \wordhighlight{DevOps}.

        \subsubsection{DevOps Definition}
        DevOps is a set of practices in software development hat aims to increase costumer value and software quality by shortening the development life cycle through active collaboration and continuous delivery of improvements \cite{base_devops}, \cite{effective_devops}. The term \wordhighlight{DevOps} is a neologism from development (Dev) and operation (Ops). The combination of these terms is symbolic for the tighter collaboration between the development and operations team, which were previously strictly separated. Therefore, DevOps is considered more than just software development principles, it is called a mindset and company/development culture. Shorter development times and closer collaboration are goals of agile software development paradigm. DevOps builds upon the guidelines and goals of agile and offers additional workflows and tools for software development to increased user experience value \cite{azuredevops}, \cite{effective_devops}.

        \subsubsection{Principles of DevOps}\label{ssec::devops_princibles}
        DevOps builds upon the agile principles. It extends the \wordhighlight{\acl{XP}} approach by applying its principles to the operations and infrastructure aspects of the application \cite{effective_devops}. The goal is to provide a structured and comprehensive process from coding up to application monitoring. This process is referred to as a workflow and the minimal steps in an exemplary workflow are shown in table~\ref{tab::devops_steps}~\cite{base_devops}.\newline

        \begin{table}[!h]
            \centering
            \begin{tabularx}{0.85\textwidth}{llX}
                \multicolumn{2}{l}{Step} & Description \\ \hline\hline
                1 & \textbf{Coding}& Code development \& review and source control.  \\
                2 & \textbf{Building}& \acs{CI} build and build status.  \\
                3 & \textbf{Testing}& \acs{CI} testing and testing feedback.  \\
                4 & \textbf{Packaging}& Bundle and package to a central registry.  \\
                5 & \textbf{Releasing}& Release management, approvals and automation.  \\
                6 & \textbf{Configuring}& Infrastructure configuration and management.  \\
                7 & \textbf{Monitoring}& Applications performance monitoring.  \\
            \end{tabularx}
            \caption{DevOps workflow steps.}
            \label{tab::devops_steps}
        \end{table}

        \noindent With such a workflow come tasks that are the core procedures in a DevOps environment. These tasks include configuration management, release management, \ac{CI}, \ac{CD}, \ac{IaC}, test automation and application performance monitoring~\cite{azuredevops}.\newline
        For the scope of this work, only solutions from configuration management, \ac{CI} and \ac{IaC} will be used to enable a homogeneous development environment. The configuration management is about versioning the applications runtime environment and how to set it up. It determines on how the application interacts with the system and external interface services. Common configuration parameters are the compiler/interpreter version, resources and permission settings as well as domain-names or \acs{IP}-addreses of other (external) services \cite{base_devops}. A DevOps principled architecture has multiple deployment stages. A staging and live environment is considered minimal base of any such projects. Adding a local environment increases the overall amount of configuration effort, requiring a reliable, efficient way to manage it. This is closely related to the \ac{IaC} principle, which describes the provisioning process of computing resources. Tools like Ansible, Puppet and Terraform can automatically create and set up new cloud \ac{VM}s specified in a playbook. Playbooks are structured policies written in a custom language which are also stored in a \ac{VCS}~\cite{ansible2020}, \cite{azuredevops}. These files describe how the host operating system is configured. Changes on these files can be handled the same way as regular code, starting with a pull request, reviews, test and approval of the changes. The \acl{IaC} principle will later be used to properly handle the setup of development containers, next to the \acs{CI} tools wich are described in Section~\ref{ssec::ci_cd}.

    \subsection{Container and Microservice Concept}\label{ssec::microservices}
    The following Section provides a basic understanding about the concepts of Microservices, how they are implemented and what possibilities they bring with them and what limitations they have. In this context, different strategies of visualization are explained, which are used in Section~\ref{sec::solution_concept}.

        \subsubsection{Fundamental Idea of Microservices}
        The fundamental idea of \ac{MSA} is to have small self-contained, independently deployable software applications. Connections between these applications make it possible to provide a greater service with an extensive range of functions \cite{micro}. An exemplary structure of a microservice cluster is visualized in figure \ref{fig::micro}. It shows multiple Microservices that each expose one functionality to the outside world. Application end-points such as a \ac{UI} provide a broad function range by internally calling multiple Microservices. In the event of one application failure, only that specific functionality becomes unavailable, the remaining system keeps operational. Because of this architectural design feature is advised that, while working with data, each microservice has its own \ac{DB}. This prevents having a central \acl{DB} becoming a single point of failure. Another core feature of Microservices in its scalability. If the load on one part of the service increases, new instances of that application can be deployed to balance to load across all instances \cite{micro}. This concept requires a fast and reliable process to create new application instances.\newline
        The installation and setup process of new hardware can be a time-consuming task. To reduce the amount of time it takes to create new application instances, the software industry uses the concept of virtualization.

        \begin{figure}
            \centering
            \includegraphics[width=0.55\textwidth]{monolithic-vs-microservices_altered.png}
            \caption{Structure of Microservices - [Altered], \\\textit{Source:~\cite{redhat_micro}}}\label{fig::micro}
        \end{figure}

        \subsubsection{Virtualization and Containerization}
        Virtualization is an abstraction layer. The physical hardware (called host) runs a hypervisor that allows the execution of (multiple) virtual machines (called guests) which act like a regular computer~\cite{vmbasics}. This approach allows the usage of heterogeneous hardware without an impact on the guest operating systems thanks to the abstraction provided by the hypervisor. Without the need for specialized hardware and the dynamic allocation of resources, efficiency is also increased~\cite{redhat_venv}. Additionally, virtual systems can be managed more easily because fundamentally they are just one big file on the host's storage device. They can be created on command, cloned and deleted without the configuration steps of a physical system. In the enterprise industry it is common to use this flexibility to start additional \ac{VM}s on high load. According to a study by the \ac{IDC} more than 80\% of data center workloads are virtualized~\cite{virtualaddoption}. Virtualization comes with the benefit of security. The majority of hypervisors strictly separate the host and the guest system. The guest system is not allowed to use the hosts resources and access its files unless it is explicitly configured to do so. Compromising a \ac{VM} does not affect the host or any other \ac{VM}s~\cite{vmbasics},~\cite{redhat_venv}.\newline
        Full guest virtualization emulates a complete \ac{OS}, including the kernel, system libraries and even the majority of hardware devices. This abstraction comes with a performance penalty called overhead~\cite{vmbasics}. A supposedly more lightweight approach of virtualization is called containerization. Studies by Ericsson Research, Nomadic Lab~\cite{ieee_perfomance} and the Zhengzhou University~\cite{zhengzhou_university} conclude in fact that container based solution provide better performance especially in disk \acs{I/O} and network \acs{I/O} bound scenarios. Containerization focuses on the isolation of one application process in a virtual runtime using control groups and namespace technologies~\cite{cgroups}. Unlike \ac{VM}s, system and kernel functions are not virtualized and are passed through to the host machine. The result is a reduction of overhead and the ability to run additional application instances compared to \ac{VM}s with the same amount of compute resources. Figure~\ref{fig::vm_docker} visualizes these differences between these approaches. The left side shows a traditional \ac{VM} based approach, on top of the host \ac{OS} runs a hypervisor which provides three full guest \acl{OS}s each with one application. Each gust is fully isolated and with its own kernel, \ac{OS} and runtime libraries. The container based solution only needs the host \ac{OS} and provides multiple application instances with shared libraries and runtimes in separated, isolated namespaces.\newline
        Apart from the performance benefits, the presumably main advantage of containers is their scalability~\cite{cintainer_scale}. Which makes them an adequate fit for Microservices. The most popular container based virtualization solutions are Docker, Podman and LXC.

        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{docker-vm-redhat.png}
            \caption{\ac{VM}s compared to Containers, \\\textit{Source:~\cite{redhat_pic}}}\label{fig::vm_docker}
        \end{figure}

        \subsubsection{Usage of Containerization in Microservices}
        As described above, microservices are small, bounded applications that communicate with each other. To follow the principles for loose coupling between applications the communication should be performed over a programming language independent protocol. Typical \acs{IP} network protocols used are \ac{REST}, WebSockets or GraphQL~\cite{micro}. These loose coupled applications have the advantage that they can be development simultaneously from different teams as well as upgraded and replaced independently. As a result, applications can be developed much faster and more flexibly, following the principles of agile development~\cite{micro},~\cite{redhat_micro}.\newline
        The usage of containers brings this speed and flexibility even further. Containers provide a consistent, isolated yet flexible runtime for applications \cite{micro_container}. Applications are packaged within known good runtimes. This reduces the setup time of the deployment and eliminate host specific errors. New application instances can be started without additional configuration. As a result of the successful concepts, tools like Docker Swarm and Kubernetes have been developed that can scale distributed applications in a managed dynamic, even automatic way.\newline
        \noindent Packaging and eventually even deploy the application introduces additional work for developers that was previously the task of the operations team. As already stated above, the developer and operations team are not separated in a DevOps culture. The new focus places value on team communication, flexibility and autonomy provided by the automation and support of as many steps as possible between the development and operation workflow~\cite{effective_devops}. Eliminating manual tasks allows developers to focus on the actual application development. One of the main concepts in this process is the usage of \ac{CI} and \ac{CD} workflows.

    \subsection{Continuous Integration and Continuous Delivery Concepts}\label{ssec::ci_cd}
    \acl{CI} and \acl{CD} are two working concepts that are particularly well known in \ac{MSA}. Since every service provides only one part of the overall application functionality, it is uncertain what effects a change in one service has to other services and the wohle application.

    \subsubsection{\acl{CI}}
    \acl{CI} is a practice where new code is regularly integrated into the main code branch. Instead of having isolated functional branches that are worked on independently for months, changes flow back regularly into the main code branch to ensure it is free from conflicts and errors. Especially in interconnected services, it is indispensable to ensure that a change in one service do not have an unintended effect on other services. \ac{CI} systems can perform automatic tasks, called jobs, on the code when it is checked into the repository. These tasks can ensure a specific code-style, run Unit/\acs{API}-tests and can also run integration tests against other services. If a task fails, the corresponding developer is notified and the changes are not incorporated into the main development branch. In case of an error it becomes easier to identify the source error, due to these small and continuous integrations. Accordingly, errors and their causes can be identified more quickly and resolved without delay \cite{azuredevops}.\newline
    Popular services that provide \acl{CI} functionality are \wordhighlight{Jenikns}, \wordhighlight{Travis}, \wordhighlight{CircleCI}, \wordhighlight{GitLab \ac{CI}} and \wordhighlight{GitHub Actions}. These services differ in their function range, type of job configuration, the level of control, whether they can be self-hosted and the pricing models respectively the number of free jobs.

    \subsubsection{\acl{CD}}
    \acl{CD} on the other hand is a practice to bring these regular updates quickly and automatically into production. To ensure product quality, it is best to have multiple deployment environments, as described in Section~\ref{ssec::devops_princibles}. Each new version is automatically deployed to a test or development environment where it undergoes automatic or manual testing. Common test types are security scans, load- usability- and acceptance tests. If all tests are successful, the version is promoted to the next deployment environment. In case of an error, the version is discontinued, and the developers are notified about it. Only if all previous environments have not revealed any errors, the version will be transferred to production as a new release. \ac{CD} is an extension of the \ac{CI} principle and performs the next logical step in the software development workflow, accordingly \ac{CI}/\ac{CD} are often used together \cite{azuredevops}.\newline
    Figure~\ref{fig::cd} visualizes a complete exemplary \ac{CI}/\ac{CD} pipeline. It is typically built from multiple stages, where each stage bundles one or more related tasks. A stage gets triggered by an event like code check-in or a previous a previously successful stage. Common tasks are code style tests along unit tests, automatic compilation or packaging and the deployment to a test system.\newline
    Both \ac{CI} and \ac{CD} are practice to automate tasks that have previously been performed manually. Integration testing and packaging have both moved to a dedicated, autonomous system, giving developers and operators more time for other activities.
    \input{inc/tikz_cd.inc}

    \subsection{Relevance of Modern Development Techniques}
    The fundamentals explained above are becoming more and more relevant with the increasing adoption of agile principles, even outside the software industry. The annual agile report showed a significant growth in agile adoption form 37\% in 2020 to 86\% in 2021. Key reason for adopting agile practices seem to be enhanced ability to manage changing priorities and increased speed of software delivery. The same reasons for adopting agile also apply for investing in culture. Over 70\% of all respondents, in the annual survey, are executing or planning a DevOps initiative. One of the biggest challenges in implementing DevOps seems to be inconsistencies in processes and practices, this result is also reflected in the fragmented and heterogeneous processes described in the next section. \cite{agilereport2021}

\section{Analysis of the current State of Development Environments}\label{sec::problem}
The following Section will display the current status of a typical \ac{DevEnv}, point out its problems and provides approaches for possible solutions.

    \subsection{Current State of Developer Environments}
    Software development involves a broad variety of tasks. Depending on the project, tasks can range from web development, desktop or mobile application development, embedded system development, data analysis or the pure maintenance of one of these variants. Each of these software development fields has its own workflows and requirements for the actual development setup. Even within these specialties, there are different requirements, depending on the scope, size, power and general capability demand for the project. Accordingly, development environments setups can differ greatly from one another. In order to limit the problem scope and possible solutions, this work will only refer to web services that are built on a Microservice architecture. Nevertheless, parts of this work can also be adapted to other types of development.\newline
    Despite the increasing use of web-based \ac{IDE}s such as \wordhighlight{CodeSandbox}, \wordhighlight{StackBlitz} \wordhighlight{Codespaces} and \wordhighlight{Gitpod}, native development environments are still primarily used. This is shown by the StackOverflow Survey, according to which Windows is the operating system primarily used by professional developers, followed by macOS and Linux~\cite{stackoverflow2021}. Accordingly, developers need a modern computer on which all the required applications must be installed and set up locally. Depending on the application area, this may include some applications for which license costs may have to be paid and which must be kept up to date after the initial installation. These are necessary supporting processes in software development, but they can cost a lot of time and effort and therefore money without producing any actual progress/value. This is especially true if there are many developers in a large company or frequently changing developers. Once the environment is running developers can start to code and contribute to the project. Programming is about changing things, changes can break things. The developers of GitHub created a series of scripts just to get developers to a working environment in less than a day. Because of frequent error these scripts included a global clean option (called \code{--nuke-from-orbit}) to reset the environment the initial known good state~\cite{githubblogcodespace}.\newline
    Such a lokal working environment gives developers lots of control over their development environments, compared to the functionally restricted web-based environments, but it requires much effort and results in an error-proneness's environment as the next section shows.

    \subsection{Common Issues in Modern Development Setups}
    The current congestion of development environments, as described above, is a collection of different configurations that each developer has spent a lot of time setting up to suit their personal preferences. Only when this has been set up for the respective project can developers begin with their actual work. However, their work may be restricted by the chosen software architecture and interrupted if changes in the code cause problems with the development environment. The following section describes the most common issues with local development environments.

        \subsubsection{The initial setup}
        Newly recruited developers or those changing projects need time and support to settle into the new project. They are not familiar with the code base and instructions on how to set up the environments correctly. Of course, good code documentation helps, as long as it is available, up-to-date and detailed enough. Specific software packages such as \ac{DB}s, interpreters or compilers need to be installed and configured in the correct versions to make the local environment operational. Depending on the project, this can be very extensive and require a lot of time and support from other team members. Setting up a NodeJS project is considerably simpler thanks to the included package manager \wordhighlight{npm} than it is with C++ or PHP projects. Further problems that arise from the configuration of the respective programming language are discussed in more detail in the next point. \newline
        Some of this initial work can be automated with scripts, but these must be created and maintained for each operating system used. These can be a time saver as long as they don't break due to moved download links or unavailable files.

        \subsubsection{Dependency Management \& Configuration Shift}
        Once the development environment is set up it is a crucial job to maintain it in such a way that developers can do their real work and contribute value to the product. Some programming languages frameworks are able to keep dependencies consistent across multiple systems without much effort thanks to their integrated toolbox helpers that come with them. NodeJS installs all the required dependencies locally in the project folder and keeps a precise record of their versions by means of a \code{package-lock} file. Other languages either come without any package manager at all or their package manager installs dependencies globally. The existence of tools like pythons \code{venv} package for creating virtual isolated environments for project dependencies proof that dependency management is in fact a problem in local development environments~\cite{pythonvenv}. Even the local package installation approach of NodeJS do not solve the problem of developing against multiple versions of the NodeJS framework. Testing a new major version of NodeJS can break existing project configurations. Additional tools like the \ac{NVM} have to be used in order to run multiple NodeJS versions. It becomes particularly problematic when developers are working on several projects at the same time and these have different dependency requirements. This issue is related to runtime dependency management. In a microservice architecture, the service dependencies are added to this.\newline
        In a microservice architecture, there are several granular services, each of which provides functionally related and bounded endpoints. Each endpoint can be considered to be a public \ac{API}, even if the service is just accessible within the of the overarching program. Communication between these endpoints is called inter-service communication. Maintaining compatibility between services becomes a challenge especially with many microservices or a service mesh \cite{micro}. If a team member changes the interface of one microservice it will affect other services and the developers responsible for these services need to be notified and respond in order to avoid unexpected errors. Tools like Swagger for applying the OpenAPI-Specification can help with these challenges. Nevertheless, the API versions used must be defined and configured. The management of inter-service communication has a direct impact on the testing possibilities, which are discussed in the next section.\newline
        In ideal environments, the local setup would always work as long as no changes are made. But there are always changes, \ac{OS} and security updates, changes to the dependencies to test a new version or the temporary change of the application port for a side by side comparison. Even in the application, the modification of the database schema by one developer can cause a broken environment for other developers. These slide changes over time are called configuration shift. No system is identical to another. For this reason, the popularity of tools such as \wordhighlight{Terraform}, \wordhighlight{Puppet} and \wordhighlight{Ansible} that implement the \ac{IaC} principle and rely on the automated creation and configuration of \ac{VM}s instead of manual creation is increasing in the cloud sector. Each instance of a machine is configured in an exact consistent way a human being could not do. If a \ac{VM} behaves abnormally, it is tern down and recreated. However, local environments are not considered disposable, they are personalized and therefor hard to automatically create and maintain. The tools mentioned above help to keep the system in a consistent good state, but do not help much with the many changes within the application. Well-known software practices such as database migrations and test suites are required to minimize errors within applications, but testing is a particular challenge in a microservice architecture.

        \subsubsection{Increased Testing Effort}
        The goal of tests is to verify the behavior of the system under the given conditions. It is a crucial practice to ensure product quality and high custom value \cite{azuredevops}. Functional tests can be categorized into the four stage shown in table~\ref{tab::tests}. According to the test pyramid, unit tests are the tests with the smallest scope, but which should be implemented most heavily. They ensure the correct behavior of a function or a class and only relay on the code to be tested itself. After the compiler or linter, they are the erliest stage to detect errors. In a \ac{TDD} environment the tests are even written before the application code itself to ensure the application meets the project requirements. This type of testing remains unchanged in a microservice architecture as each service can still have its own unit tests. Due to the number of applications, though, \ac{IPC} in a microservice architecture increases significantly, making integration and higher testing scenarios more complex \cite{microtest}. \newline
        Integration tests should verify that two services can interact with each other. In order to perform these it is necessary to is to run both services \cite{azuredevops}. Triggering an \ac{API} call and verifying its result shows integration errors, but makes them as complex as end-to-end tests thanks to the configuration and orchestration of multiple services \cite{microtest}. While developers can use tools such as \wordhighlight{Postman} to check the results of an \ac{API} call, this does not guarantee that two services can actually communicate successfully. Integration tests can either be done in \ac{CI} where an error can only be detected after code has already been committed, pushed and \ac{CI} tasks completed, or locally with immense effort for the entire configuration of all services. Due to the high rate of inter process communication, the use of tracing tools may be necessary to isolate an inter service. Integrating these programes into \ac{CI} and accessing them locally is another significant technical task. This results either in very late error detection, which slows down development and makes it more expensive, or in an individual, local environment that is difficult to maintain and prone to errors. Both scenarios leave developers with insufficient testing capacities for efficient software development. Although contract testing for compliance with shared \ac{API} specifications can be used for integration testing, this merely postpones the problem to a later test stage \cite{microtest}. \newline
        Entire application tests, called end-to-end tests, are extensive and slow. They need a lot or all services to be deployed cover whole business logic operations \cite{microtest}. They should not be used thoughtful and performed on a testing or staging environment However, these only form a small part of the test scope and do not fall into the category of increased testing effort in a microservice architecture.

        \begin{table}[]
            \centering
            \begin{tabularx}{0.9\textwidth}{lX}
                Test-Type & Description \\ \hline\hline
                Unit test& Test a small part of a service, such as a class.\\
                Integration tests & Verify that a service can interact with infrastructure services \\
                Component tests & Acceptance tests for an individual service. \\
                End-to-end tests & Acceptance tests for the entire application.
            \end{tabularx}
            \caption{Different types of tests, \\\textit{Source:~\cite{microtest}}}\label{tab::tests}
        \end{table}

        \subsubsection{Issues Caused by Heterogeneous \acs{DevEnv}'s} % OS specific errors
        \subsubsection{Additional Effort for Developers}
        % VPN Zugang/keys und co. KÃ¶nnen in container gepackt werden

    \subsection{Propsed Solution}\newpage
    % \textit{Notes for me:}
    % \begin{itemize}
        %     \setlength\itemsep{0em}
        %     \item Lokale Entwicklung
        %     \item API Poking
        %     \item AWS vs local
        %     \item Unit tests
        %     \item terminal hell
        %     \item log hell
        %     \item CI Tests
        %     \item End to end tests
        %     \item Zeit/Kosten
        %     \item Config aufwand. Fokus auf code nicht auf config.
        %     \item Reproduzierbarkeit
        %     \item Umgebung ist managebar
        % \end{itemize}

\section{Solution Concept of DevContainers}\label{sec::solution_concept}
    \subsection{Pre-requirements for DevContainers}
    \subsection{Description of a Conceptual Environment}
    % Threat them as we threat servers. Throw away if they break
    \subsection{Available Tools and Functionality of the Technologies Used}\label{ssec::toolsused}
    The following Section provides a deeper dive into the technologies that implement the above concepts and why they are chosen. These inner workings are relevant for later chapters as they have a direct influence on the development of the solution concept and their limitations.
        \subsubsection{Docker}
        Docker is the currently most popular containerization platform that is supported on Linux, Windows and macOS~\cite{docker_share}. It utilizes the hosts Linux kernel and its cgroups functionality for resource isolation. On none Linux host systems Docker virtualizes the kernel via a hypervisor. Docker was chosen for its wide usage, extensive documentation and its platform independency, which enables usage in local and productive operations.\newline
        Docker allows the isolated execution of applications within a container in a well-defined state without affecting the host. The initial state of a container is defined in a disk-image that bundles all the software, libraries and configuration files needed. Images are build based on a \wordhighlight{Dockerfile} that contains sequential, imperative instructions on how the container \ac{OS} should be configured. An exemplary Dockerfile can be found in the appendix in Listing \ref{code::docker}. Common steps are the \code{COPY} command to add files form the host to the container and the \code{RUN} command to execute shell commands within the container. Each instruction that changes files adds another layer to the overlay-filesystem that is used by Docker. Effectively making each layer a read only filesystem once it is written. At runtime a file-lookup is performed in the top filesystem layer, if the file is not found lookups in the layers below follow. This allows the reuse of layers, because changed versions of files are simply pushed into a new layer and overshadow the original files. Docker images are distributed via a Registry to other systems. The company that develops Docker also provides an official and public Docker Image Registry called DockerHub. It provides official images of common applications such as \acl{DB}s and webservers as well as base images for own projects \cite{docker2020}, \cite{dockerdocs}.\newline
        When a container is started, the program specified in the image is executed, and the container runs until it is stopped. Each container is treated as an independent full-fledged computer with its own \ac{OS}. For this reason, each container has its own \acs{IP} address and is by default not reachable from the host system. The user has to explicitly expose a container port to the host system in order to access the application inside the container. Each container not only has its own \acs{IP}, but is also on its own software defined network, which is managed by Docker and even uses a Docker internal \ac{DNS} server. Containers within a defined network can communicate with each other by their (host-)names and provide their functionality outside the network by exposing their published ports to the host \ac{OS} \cite{docker2020}.\newline
        Unlike \ac{VM}s, the containers guest \ac{OS} is not maintained, as containers are considered disposable. Instead of updating the software within the container, it is simply replaced by a newer one from an updated image. All changes and new files within the container will be lost. To avoid losing user data, private keys and the like, these can be stored on the host system for persistence and made available in the container via mounts. A mounted directory behaves like new top layer in the overlay filesystem. Docker distinguishes between volume-mounts and bind-mounts. Volumes are managed by Docker, have a unique name, can easily be shared between containers and are transferable between hosts. With bind mounts a specified directory from the host is mounted to the container. All filesystem attributes and file properties are passed to the container, the management is done of that mount is up to the user \cite{docker2020}, \cite{dockerdocs}.\newline
        Alternatives to Docker are \wordhighlight{Podman}, \wordhighlight{Rkt} and \wordhighlight{LXC}, wich Docker was originally based on. The downside of these tools are their lower integrability and lack of multi-platform support.
        \subsubsection{Version Control, \acl{CI} and \acl{CD}}
        A \acl{VCS} (\ac{VCS}) is an essential tool in the software development industry to keep track of which change was made when and by whom. Git became the defacto standard over the last 10 years and is used in this project just like by over 93\% of all developers according to the StackOverflow Developer Survey 2021~\cite{stackoverflow2018}. Git allows collaborative, distributed work in own branches without affecting others. Changes can be merged back into the main branch after an optional review and made available for all developers. A self-hosted instance of GitLab is used to coordinate teamwork, serves as a central code repository and for handling task management. Its adaptability, extensive integration options and built-in \ac{CI} tools provide all the necessary software lifecycle programs from a single service bundle. Its \ac{CI}/\ac{CD} functionality is used to automatically test, build and deploy the code to a testing environment. The built-in private container registry allows the creation of always up-to-date Docker images and make them accessible to the developers, so that this task does not burden the local computers. Alternatives are \wordhighlight{GitHub} and \wordhighlight{Bitbucket}, both of which offer code hosting, issue management, \ac{CI} and \ac{CD}. Standalone solution for \ac{CI}/\ac{CD} are \wordhighlight{Travis}, \wordhighlight{CircleCI} or \wordhighlight{Jenikns}.

        \subsubsection{Editors}
        Editors are the primary working tool of developers when writing code. The variability and the number of different editors is great. Well known examples are Visual Studio, XCode, Atom, Sublime, Eclipse, Emacs and VIM to just name a few. There is usually a distinction between simple text editors and \acl{IDE}. While text editors are quite simple and only provide basic functionalities like syntax highlighting, \ac{IDE}s are much more comprehensive with powerful IntelliSense suggestions, built-in project management, \ac{VCS}, debugger, graphical representations and build-tools. The choice of the editor is a personal decision for most developers, and they are customized according to their preferences. For this reason, this work does not require a specific editor for DevContainers, but gives a recommendation that will be used for the rest of the work.\newline
        \ac{VSCode} is a free platform independent editor from Microsoft with extensive extendibility wich is used by over 70\% of all developers on StackOverflow~\cite{stackoverflow2021}. One of its key beneficial features for virtualized work environments is its Remote Development function. This, officially provided, extension provides the comfort of a graphical \ac{UI} while running the code, the application and auxiliary processes like debugger on another machine. As Figure \ref{fig::vscoderemote} shows, \ac{VSCode} connects to the remote machine, installs a server instance of the editor that manages access to the file system and the execution of other processes, and communicates with the local \ac{VSCode} instance for comfortable access to these functions \cite{vscodedevcontainer}. The functionality to make remote processes available locally via port forwarding is useful to allow the usage of local programes with these remote services. This type of remote development works for \acs{SSH} connections, the \ac{WSL}, and on containers. \ac{WSL} is a built-in feature to provide a Linux environment on Windows without the need of a full \ac{VM}. The implementation of Docker for Windows is build upon this functionality. Details on the use and configuration of this function are described in detail in Section \ref{ssec::imp_process}. All these functions can be used without \ac{VSCode} by using remote filesystem mounts, port-forwarding or terminal editors, but it requires more configuration effort.
        \begin{figure}[]
            \centering
            \includegraphics[width=.95\textwidth]{vscode-remote.png}
            \caption{Structure of \ac{VSCode} remote Development setup \\\textit{Source:~\cite{vscodedevcontainer}}}\label{fig::vscoderemote}
        \end{figure}

        \subsubsection{Other}
        In addition to the tools mentioned above, other auxiliary programes are used. The use of scripts is intended to simplify the work and make recurring tasks easier. \wordhighlight{Bash} scripts can be used natively on Linux and macOS, the installation of Git also brings Bash support to Windows. This allows operations to be performed on multiple repositories, initial settings to be set and complicated build instructions to be simplified. \wordhighlight{Docker Compose} follows the same approach and facilitates the management of multiple interrelated Docker projects. It is a program for orchestrating multiple Docker services. It ensures that interdependent services are started in the correct order and that all necessary persistent volumes are created \cite{docker2020}, \cite{dockerdocs}. The configuration for the orchestration with compose is typically specified in a \code{docker-compose.yml} file that follows the \ac{IaC} principle and is tracked in \ac{VCS}. An optional feature is to further organize the logs from the docker containers. Although Docker Compose offers color-grouped log outputs per container, these can become overwhelming if there are many containers. Since the Docker stack is used anyway, enterprise log aggregators and analysis tools like \wordhighlight{Grafana} or \wordhighlight{Elastic Search} can be used to get a persistent and personalized log dashboard. In this concept, a Grafana container is used that gets its logs from the \wordhighlight{Loki} log-shipper via a Docker plugin.

    \subsection{Possible Implementations}
    \subsection{Strengths, Weaknesses and Limits}\label{ssec::limits}\newpage

\section{Exemplary Prototype Implementation}\label{sec::solution_code}
    \input{inc/tikz_arch.inc}
    % \input{inc/tikz_arch_alt.inc} % Alternative
    \subsection{Current State and Goal}
        \subsubsection{Project Stating Point}
        \subsubsection{Target State and Goal of it}
    \subsection{Implementation Approach}\label{ssec::imp_approach}
    \subsection{The Implementation Process}\label{ssec::imp_process}
    \subsection{Encountered Challenges and Limits}
    \subsection{Final State} \label{sec::final}
%     \begin{lstlisting}[language=bash, frame=single, backgroundcolor=\color{codebg}]
% Here is the codes format i want to show in latex
%     \end{lstlisting}


\section{Performance Evaluation and Analysis}\label{sec::eval}
\subsection{Metrics and how to Evaluate}
    \subsection{Evaluation and Results}
    \subsection{Discussion of Evaluation}

\section{Future Potential and Outlook}\label{sec::outlook}
\section{Conclusion}\label{sec::conclusion}

% STATS
% 6750 Words
% 38.000/44.600 Characters

\newpage
% Anhang
\lhead{Appendix}
\renewcommand{\thesubsection}{\Alph{subsection}}
\pagenumbering{Roman}
\setcounter{page}{\value{lastroman}}
\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

% Abbreviations
\input{inc/shorts.inc}
\newpage

%Code
\input{inc/code_docker.inc}
\input{inc/tikz_arch_dir.inc}
\newpage
\listoffigures
\listoftables
\lstlistoflistings{}

%Bibliographie
\addcontentsline{toc}{section}{References}
% \bibliographystyle{alpha}
\bibliographystyle{IEEEtranSA}
\bibliography{bib/sources}

% Authorship
\include{inc/ensure.inc}

\end{document}
% END DOC/EOF
